Kubernetes:
Containerization --> Docker, Rocket(Rkt),Container-d
   Containerization involves writing/modifying Dockerfiles and using those files
   to create images and shipping the images to image registries.  
   From the registries these images can be distributed  

  We could also use Containerization softwares like Docker to deploy and manage Containers

Container Orchestration Tools --> 
   Docker Swarm,
   Kubernetes,
   OpenShift

Kubernetes:
  10 years 
  July 2015  --- 2023-2015=7years 

kubectl create/delete/apply/get/describe/run/expose 
   authentication  = .kube/config  
   authorisation   = RBAC  

kubernetes architecture:
controlPlane:
  apiServer
  etcd  
  scheduler  
  controllerManagers   
workerNodes:
  kubelet   
  container runtime [Container-d]
  kube-proxy
kubernetes-client:
  kubectl  
      kubectl create/delete/get/describe/apply/run/expose 
      kubeconfig [.kube/config ] file will authenticate 
                                 the caller admin/Developer/Engineer  
  ui  
  api  

kubernetes security - RBAC:
  Developers [ Paul, Joyce, Chidi ] 
  Engineers  [ James, Isaac, Janet ] 

authentication via kubeconfig : 
authorisation via RBAC:

Installation:
============
Local K8s Cluster(Single Node K8s Cluster)
------------------------------------------
   minikube
       choco install minikube
       brew install minikube 
       minikube start  
   Kind   = 
   Docker Desktop 
      https://docs.docker.com/desktop/kubernetes/
  POC = 

Multi Node Kubernetes Clusters:
================================
1. Self Managed Kubernetes [k8s] Cluster = IaaS--EC2  :
    kubeadm --> We can setup multi node k8's cluster using kubeadm.
    kubespray --> We can setup multi node k8s cluster using kubespray
     (Ansbile Playbooks Used internally by kubespray).

     controlPlane [apiServer, etcd, scheduler, Controller Managers] 
      and 
     workerNodes [  kubelet, container runtime-Container-d, kube-proxy]  
  are managed by the Admin/Kubernetes/DevOps Engineers

2. Managed k8s Cluster  (Cloud Services) = PaaS  : 
   The controlPlane and all it components are managed by the Cloud provider 
   EKS --> Elastic Kubernetes Service(AWS)
   AKS --> Azure Kubernetes Service(Azure)
   GKE --> Google Kubernetes Engine(GCP)
   IKE --> IBM K8s Engine(IBM Cloud)

Kubernetes Cluster = k8s  

3. KOPS: is a software use to create production GRADE/ready k8s in AWS and  
         azure for the kops beta version  
         highly available kubernetes services in Cloud like AWS.
            KOPS will leverage Cloud Sevices like:
              vpc, 
              AutoScaling Groups, 
              LoadBalancer, 
              Launch Template/configuration
              ec2-instances nodes [workerNodes and masterNodes]  
   kops create cluster --name mycluster --az us-east-2b nodes-4 master 3    

 iam role/user  
 

Rancher: - Using Rancher we can deploy both managed and self managed k8s
           Rancher serves as a glass to access and manage multiple k8s  
           from the dashboard [UI]  - rancher dashboard 
           authentication and authorisation: EKS/AKS/GKE/IKE 

Ticket 001:
  Setup a multi nodes self Managed kubernetes cluster using kubeadm.
  requirements -- 
1. check the kubernetes official documentation  
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

2. check the company's documentation for kubeadm setup  
https://github.com/unitedcoresystems/Bootcampclass-2023/tree/main/Package_management/Kubernetes/Kube-adm

kubeadm join 10.0.0.20:6443 --token pqupgp.skh0fxwca870hh3n \
        --discovery-token-ca-cert-hash sha256:72f30fc8be50aaf665f7837834736cfbfa80c00cdeb64326c5ddd4cafb7792d6

docker resources/objects use to deploy applications:
  Dockerfiles/images/networks/volumes/docker-compose.yml/etc.  

kubernetes resources/objects used to deploy application:
    Pod or   
    controllerManagers:
    Replication Controller
    ReplicaSet
    DaemonSet
    StatefulSets
    Deployment
    Volume
    Job  

Exposing/accessing applications = Service Discovery:
    Service Types:
    ClusterIP
    NodePort
    LoadBalancer
    ExternalName  
  ingress 
  networkPolicy 


34.238.233.46


Namespace:
  It is virtual cluster inside your cluster 
  [ dev / uat / prod ], [sales, accounts, cs, payroll]
  pros:
    isolation  
    permissions 
       dev  - Developers
       prod - Engineers
    resource utilisation  
       dev  - cpu=5Gi mem=1000Mi 
       prod - cpu=25Gi mem=8000Mi 
    performance
       High priority   


RBAC - Security:
  NameSpace
  Role 
  RoleBinding  
  ClusterRole  
  ClusterRoleBinding
  ServiceAccounts  

Namespace:
  fintech / 
  ecommerce 

kubernetes uses the kubectl client or the UI to run workloads.
  kubectl get namespace
  kubectl get ns

NameSpaces via team:
   Developers = dev-ns   
   QA         = qa-ns
   Engineers  = prod-ns   

NameSpaces via projects:
    Customer care  
    Sales 
    Claims 
    Refunds 

https://kubernetes.io/docs/setup/best-practices/cluster-large/
   No more than 110 pods per node
   No more than 5,000 nodes
   No more than 150,000 total pods
   No more than 300,000 total containers

kubectl get namespaces
kubectl get ns   

-#Create Name Space Using Imperative Command
    kubectl create namespace <nameSpaceName>
    kubectl create namespace dev  

declarive approach
-# Using Declarative Manifest file 

apiVersion: v1   
kind: Namespace      
metadata:
  name: prod  

kubectl api-resources | grep namespace

PODS:
====
POD --> Pod is the smallest building block which we can deploy applications in k8s.
Pod represents running process. Pod can contains one or more containers.
These container will share same network, storage and any other specifications.
Pod will have unique IP Address in k8s cluster.

Pods
 SingleContainerPods --> Pod will have only one container. 98%
 
 MultiContainerPods(SideCar) --> POD with two or more containers. 2%  
         application Container  
         SideCar containers:
         logMgt  container  
         utility Container [ Truck = ]

How to deploy run/execute tasks/workloads in kubernetes??
   1. Imperative  approach 
        By using commands   
   2. Declarative approach
        By using files [manifests files]  

-# Create POD Using Command

kubectl run <podName> --image=<imageName> --port=<containerPort> -n <namespaceName>

kubectl run hello --image=unitedcore/hello --port=80 -n dev  

Docker images = dockerHub other registries:
-- python-web-app
   nodeweb-app 
   net-webapp 
   unitedcore/hello
   nginx 
   mysql  
   mongo  
   jenkins  
   sonarqube  
   nexus 


To deploy workloads in kubernetes we must have a running cluster.
We deploy applications in isolated virtual clusters/environments call

Use the declarive to deploy workloads in kubernetes:
  Manifest files = kams 
Manifest files are written in yaml/yml language 
key:values pairs:
  key1: value1  
  key11: value11 
  name: Issac   
dictionary: number of key:values pairs: 
  name: Issac  
  age: 50 
  sex: male   

list:
  - name: Issac 
    age: 50 
  - name: paul 
    age: 55 
  - name: james 
    age: 60  
pod.yml
======
apiVersion: v1    
kind: Pod     
metadata:
  name: <podName>
  namespace: <namespaceName>
  lables:
    key: <value> 
    key: <value> 
spec:
  containers:
  - name: containerName   
    image: imageName  
    ports:
    - containerPort: podNumber  
---
kind: Pod  
apiVersion: v1 
metadata: 
  name: myapp  
  namespace: dev     
  labels:
    app: myapp 
    tier: fe  
spec: 
  containers:
  - name: webapp 
    image: unitedcore/hello
    ports: 
    - containerPort: 80 



myappsvc--->
kubectl config set-context --current --namespace=dev

kubectl get all
kubectl delete all --all 
kubectl delete all --all -n dev     
kubectl delete pod --all
kubectl get pods 
kubectl get pods -o yaml  
kubectl get pods --show-labels
kubectl get pods -o wide
kubectl get pods -o wide --show-labels

kubectl  describe pod <podName>
kubectl  describe pod <podName> -n <namespace>

ServiceDicovery:
===============
ClusterIP is the default kubernetes service type that support communication  
within the cluster.  
kams  
service.yml

service.yml
===========
kind: Service
apiVersion: v1  
metadata:
  name: myappsvc
  namespace: dev
spec:
  type: ClusterIP  
  ports:
  - port: 80  
    targetPort: 8080
  selector:
    app: myapp

kubectl get svc -n <namespace>
kubectl describe svc  
kubectl get ep  
kubectl describe svc   
kubectl delete svc   

kubernetes Service:
  In Kubernetes Service makes our pods accessible/discoverable 
  within the cluster or exposing them outside  the cluster.
  service will identify pods using it's labels And Selector. 
  Whenever we create a service a ClusterIP (virtual IP) Address 
  will be allocated for that serivce and DNS entry will be created for that IP.
  So internally we can access using service name(DNS).


What is FQDN?
Fully Qualified Domain name. 
If one POD need access to service & which are in differnent names space 
we have to use FQDN of the service.
Syntax: <serivceName>.<namespace>.svc.cluster.local
ex: myappsvc.dev.svc.cluster.local

root@webapp:/usr/local/tomcat#
      curl -v 


IQ: what is Static Pods ?
    Static Pods are controlled by the kubelet service  

sudo vi /etc/kubernetes/manifests/file.yml  
kind: Pod  
apiVersion: v1 
metadata: 
  name: webapp  
  namespace: dev     
  labels:
    app: webapp 
spec: 
  containers:
  - name: app 
    image: unitedcore/maven-web-app
    ports: 
    - containerPort: 8080 

NB:
We should not create pods directly to deploy applications.
If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods with help of controllers which manages POD life cycle.

controllerManagers:
  ReplicationControllers 
  ReplicaSets, 
  Deployments, 
  DaemonSets  

A workload is an application running on Kubernetes consisting of a single component 
or several components that work together inside a set of pods. 
In Kubernetes, a Pod represents a set of running containers on your cluster.

Kubernetes pods have a defined lifecycle. 
For example, once a pod is running in your 
cluster and the node hosting the pod fails then pods running on the node will fail. 
Kubernetes treats that level of failure as final. 
You would need to create a new Pod to recover,even if the node later becomes healthy.

ReplicationControllers = rc   
========================== kams:
kind:  ReplicationController
apiVersion: v1   
metadata:  
  name: apprc   
  namespace: dev    
  labels:
    app: myapp 
spec:
  selector: 
    app: fe  
  replicas: 2    
  template:  #podTemplate 
    metadata: 
      name: apppod  
      labels:
        app: fe 
    spec:  
      containers:
      - name: app 
        image:  unitedcore/java-web-app
        ports: 
        - containerPort: 8080

NodePort Service:
===============
Manages external traffic in the cluster  

appsvc.yml  
----------
kind: Service   
apiVersion: v1  
metadata:
  name: appsvc    
spec:
  selector:
    app: fe  
  type: NodePort 
  ports:
  - targetPort: 8080
    nodePort: 31000 # 30000 - 32676   
    port: 80 

54.209.151.57
44.192.109.203
34.238.233.46


kubectl apply -f <filename.yml>
kubectl get rc 
kubectl get rc -n <namespace>
kubectl get all
kubectl scale rc <rcName> --replicas <noOfReplicas>

kubectl describe rc <rcName>
kubectl delete rc <rcName>


44.192.109.203:31000/java-web-app  

curl 44.192.109.203:31000/java-web-app   


ReplicaSet = RS :
==========
What is difference b/w replicaset and replication controller?

RS is the next generation of replication controller. 
Both RS and RC manages the pod replicas and state. But only difference as now is
selector support.

RC --> Supports only equality based selectors.
key == value(Equal Condition)
selector:
    app: javawebapp
    tier: fe    
    client: tesla  

RS --> Supports eqaulity based selectors and also set based selectors.  
eqaulity based:
key == value(Equal Condition)  
set based:
  key in [ value1, value2, value3 ]
selector:
   matchLabels:   -# Equality Based
     key: value
    app: javawebapp
    tier: fe    
    client: tesla  
   matchExpressions: -# Set Based
   - key: app
     operator: in
     values:
     - javawebpp
     - myapp  
     - fe  

rs.yml  = kams 
--------------
kind: ReplicaSet    
apiVersion: apps/v1    
metadata:
  name: <RSName> 
  labels:
    <key>: <value> 
spec:
  selector:
    matchLabels:
      <key>: value           
    matchExpressions:
    - key: <key>
      operator: <in /not in>  
      values: 
      - <value1> 
      - <value3>      
  replicas: 3   
  template:
    metadata:
      name: podName  
    labels:
      <key>: <values>  
    spec:
      containers:
      - name: containerName  
        image: imageName:tag    
        ports:
        - containerPort: <ContainerpodNumber>      
--- rs.yml  
kind: ReplicaSet
apiVersion: apps/v1   
metadata:
  name: webrs  
spec:
  selector: 
    matchLabels:
      app: web 
  replicas: 2   

  template:
    metadata:
      name: webapp  
      labels:
        app: web  
    spec:
      containers:
      - name: web 
        image: unitedcore/python-flask-app:2    
        ports:
        - containerPort: 5000      

node-app-rs.yml = kams 
================
kind: ReplicaSet    
apiVersion: apps/v1     
metadata:
  name: nodeapp   
spec:
  replicas: 2    
  selector:
    matchLabels:
      app: node   
  template:
    metadata:
      name: nodeweb-app  
      labels:
        app: node 
    spec:
      imagePullSecrets:
      - name: dockerhublogin      
      containers:
      - name: nodeapp   
        image: unitedcore/nodejs-fe-app
        ports:
        - containerPort: 9981      
---
kind: Service
apiVersion: v1  
metadata:
  name: websvc  
spec:
  type: NodePort  
  selector:
    app: node 
  ports:
  - targetPort: 9981
    ports: 80 
    nodePort: 32000



kubectl create secret docker-registry regcred
 --docker-server=<your-registry-server> 
 --docker-username=<your-name> 
 --docker-password=<your-pword>
 --docker-email=<your-email>


kubectl create secret docker-registry dockerhublogin \
    --docker-server=docker.io --docker-username=unitedcore \
    --docker-password=admin123  

apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred
---
---


kubectl get rs 
kubectl get rs -n <namespace>
kubectl get all
kubectl scale rs <rsName> --replicas <noOfReplicas>

kubectl describe rs <rsName>
kubectl delete rs <rsName>

kubectl scale rs nodeapp --replicas 3 

DaemonSet:
==========

What's the difference between RS and DS
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
4 nodes  =    

ds.yml  = kams 
-=====--------
kind: DaemonSet    
apiVersion: apps/v1    
metadata:
  name: <RSName> 
  labels:
    <key>: <value> 
spec:
  selector:
    matchLabels:
      <key>: value           
    matchExpressions:
    - key: <key>
      operator: <in /not in>  
      values: 
      - <value1> 
      - <value3>      
  template:
    metadata:
      name: podName  
    labels:
      <key>: <vales>  
    spec:
      containers:
      - name: containerName

hello_ds.yml   
================
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logmgt
spec:
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      name: hello
      labels:
        app: hello
    spec:
      containers:
      - name: hello
        image: unitedcore/hello
        ports:
        - containerPort: 80
=====================================

k8s--->nodes--->pods---->Containers:
  How can we deploy containerised applications in k8s?
  We use kubernetes objects to deploy workloads in kubernetes:
  1. Pods --- 
        scaling is not supported  
        lifecycle is very short  
        lacks self-healing capacities
    controllerManagers:
  2. ReplicationControllers  
       kubectl scale rc/rs/deploy  
  3. ReplicaSets
  4. Deployment
  5. DaemonSets
  6. StatefulSets 

1. Deploy an application which must have a pod running in each = ds  
     logMgt / logshipper  
2. Deploy an application with scaling capacities = rc/rs/Deployment/sts  
3. Deploy an application with scaling capacities = pod  

Master node is tainted / taint  
=============================  
  -- recommissioning / upgrades / updates / patching  


node1    Ready 

kubectl taint nodes node1 key1=value1:NoSchedule

hello-ds.yml  
===========
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: logmgt
spec:
  selector:
    matchLabels:
      app: hello
  template:
    metadata:
      name: hello
      labels:
        app: hello
    spec:
      tolerations:
      - operator: Exists
        effect: "NoSchedule"
      containers:
      - name: hello
        image: unitedcore/hello
        ports:
        - containerPort: 80

Master node is tainted / taint  
=============================  
  -- recommissioning / upgrades / updates / patching  

kubectl taint nodes node1 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node1 key1=value1:NoSchedule-   [untaint the node]

sudo vi /etc/kubernetes/manifests/

imperative  
declarative

kubectl apply -f rc.yml 

kubectl apply -f <ds-filename.yml>
kubectl get ds 
kubectl get ds -n <namespace>
kubectl get all

kubectl describe ds <dsName>
kubectl delete ds <dsName>

kubectl get/describe/delete/edit/apply/  

==============================================

Deployments  
==========
  Advantages:
     Deploy a RS.
     Updates pods (PodTemplateSpec).
     Rollback to older Deployment versions.
     Scale Deployment up or down.
     Pause and resume the Deployment.
     Use the status of the Deployment to determine state of replicas.
     Clean up older RS that you don’t need anymore.

kubectl apply deploy-app.yml :
  rollout a ReplicaSet  

---
kind: Deployment   
apiVersion: apps/v1    
metadata:
  name: <deploymentName> 
  labels:
    <key>: <value> 
spec:
  strategy:
    rollingUpdates   
    recreate  
  selector:
    matchLabels:
      <key>: value           
    matchExpressions:
    - key: <key>
      operator: <in /not in>  
      values: 
      - <value1> 
      - <value3>      
  template:
    metadata:
      name: podName  
    labels:
      <key>: <vales>  
    spec:
      containers:
      - name: containerName
---
---
app.yml  - kams
------
kind: Deployment
apiVersion: apps/v1
metadata:
  name:  webapp
  namespace: dev
  labels:
    app: be
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      name: webapp
      labels:
        app: web
    spec:
      containers:
      - name: webappc
        image: unitedcore/hello:v1
        ports:
        - containerPort: 80
---
kind: Service
apiVersion: v1
metadata:
  name: webappsvc
spec:
  selector:
    app: web
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 32000 #[30000-32676]
~



44.192.109.203:31000/maven-web-app  

curl 44.192.109.203:31000/java-web-app  


-# Deployment ReCreate
---------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello 
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hello 
  strategy:
    type: Recreate    
  template:
    metadata:
      name: hello
      labels:
        app: hello  
    spec:
      containers:
      - name: helloworld    
        image: unitedcore/hello:v2
        ports:
        - containerPort: 80   

hello-svc.yml  
-------------
kind: Service  
apiVersion: v1  
metadata:
  name: hellosvc  
spec:
  selector:
    app: hello   
  type: NodePort 
  ports:
  - port: 80 
    targetPort: 80
    nodePort: 32000 #[30000-32676]  

44.192.109.203:32000 

Update Deployment Image using command 
--------------------------------------
kubectl set image deployment <deploymentName> <containerName>=<imageNameWithVersion> --record

kubectl set image deployment hello unitedcore/hello:1 --record  

kubectl rollout status deployment <deploymentName>
kubectl rollout history  deployment <deploymentName>
kubectl rollout history  deployment <deploymentName> --revision 1  
kubectl rollout undo  deployment <deploymentName> --to-revision=1  
kubectl scale deployment <deploymentName> --replicas <noOfReplicas>

kubectl rollout history deployment hello --revision 1    

unitedcore.com/success 


ReCreate strategy  --- 
   It comes with downtime  

RollingUpdates strategy
   no downtime 

# Deployments  Rolling Update
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
       maxSurge: 1
       maxUnavailable: 1
  minReadySeconds: 30   
  selector:
    matchLabels:
      app: hello 
  template:
    metadata:
      name: hello
      labels:
        app: hello  
    spec:
      containers:
      - name: hello  
        image: unitedcore/hello:v3 
        ports:
        - containerPort: 80 



Blue Green deployment Technique : 
Canary deployment Technique : 
  youth 
  adults  
Lagos  / Douala / Accra / Dallas / London  

  users--ROW                               = 75% of traffic 
  users--Lagos[ newApplication version ]   = 25% 

Resource, requests and limits:
--------------------------

eqaulity based selectors:
  ReplicationControllers  
    key:values   
    app: myapp  
Set based selectors:
  Deployments  
  ReplicaSets  
  DaemonSets 
  StatefulSets  
  NodeSelectors 
  NodeAffinity  
key: app 
value in:
  - javawebpp  
  - myapp 

kubernetes.io     


27 Nov 2023:
===============
Resource, requests and limits:
--------------------------

eqaulity based selectors:
  ReplicationControllers  
    key:values   
    app: myapp  
Set based selectors:
  Deployments  
  ReplicaSets  
  DaemonSets 
  StatefulSets  
  NodeSelectors 
  NodeAffinity  
key: app 
value in:
  - javawebpp  
  - myapp 

kubernetes.io     

 Deployments  :
    strategy [ReCreate and RollingUpdates]
    techniques [Blue/Green, Canary]

Resource, requests and limits:
--------------------------
--------------------------
Requests and limits are the mechanisms Kubernetes uses 
to control resources such as CPU and memory. 
Requests are what the container is guaranteed to get. 
If a container requests a resource, 
Kubernetes will only schedule it on a node that can give it that resource.

Limits, on the other hand, make sure a container never goes above a certain value. 
The container is only allowed to go up to the limit, and then it is restricted.

Resource request:
---------------
A request is the amount of that resources that the system 
will guarantee for the container, and Kubernetes will use this value 
to decide on which node to place the pod. 

Resource Limit:
A limit is the maximum amount of resources that 
Kubernetes will allow the container to use.

Cluster = node1[mem=4000MB/32MB]    cpu[8000m/4000m]     : million bytes
          node2[mem=4000Mi/2200MB]  cpu[16000m/12000m]
          node11[mem=4000Mi/2200MB] cpu[16000m/12000m]
pod2.yml -- nodes/pods  
========
apiVersion: v1
kind: Pod
metadata: 
  name: unitedwebapp   
spec:
  containers:
  - name: unitedwebapp 
    image: unitedcore/united-web-app   
    ports:
    - containerPort: 8080   
    resources:
      requests:
        memory: "128Mi"
        cpu: "500m"          
      limits:
        memory: "512Mi"
        cpu: "1000m"


Commands

kubectl top pods  
results:
ubuntu@master:~$ kubectl top pods
error: Metrics API not available

kubectl top nodes   
results:
ubuntu@master:~$ kubectl top nodes
error: Metrics API not available
ubuntu@master:~$


Horizontal Pod AutoScaling  - HPA  :
==================================
POD AutoScaling --> Kuberenets POD AutoScaling Will make sure u have minimum number 
pod replicas available at any time & based on the observed CPU/Memory utilization
on pods it can scale PODS.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 

What is difference b/w Kubernetes AutoScaling(POD AutoScaling) & AWS AutoScaling?


  aws  
  AutoScaling Group = ASG
     minimum   10      20 [81%]  21
     desired   10      20 [75%]
     maximum   100 

  ScalingPolicy:
     memory utilization
       cpu -gt 80% 
       cpu -lt 40%     
     cpu utilization
difference b/w Kubernetes AutoScaling(POD AutoScaling) 
                   & AWS AutoScaling?

app2.yml  
=======
kind: Deployment
apiVersion: apps/v1
metadata:
  name: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: tesla
  template:
    metadata:
      name: webapp
      labels:
        app: tesla
    spec:
      containers:
      - name: app
        image: unitedcore/united-web-app
        resources:
          requests:
            cpu: "1000"
            memory: "256Mi"
          limits:
            cpu: "1000"
            memory: "256Mi"
        ports:
        - containerPort: 8080

error: Metrics API not available
    kubernetes addons/plugins:
      Metrics Server

Configure a Metrics Server on our Cluster4??
===========================================
https://github.com/unitedcoresystems/metric-server
git clone https://github.com/unitedcoresystems/metric-server


wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml



kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


kubectl apply -f metric-server/metrics-server-deploy.yml
=====================================================
ubuntu@master:~/tesla$ kubectl apply -f metric-server/metrics-server-deploy.yml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
ubuntu@master:~/tesla$


kubectl top pods
kubectl top nodes



metrics-server:
  nodes  
  pods 

RBAC objects:
  serviceaccount metrics-server 
     - user  
     - groups 
     - pods     
  clusterrole
     - pods/nodes [get/watch/list] 
  clusterrolebinding
     - 
  rolebinding

Commands:

  kubectl top pods -w ----> to check on the metrics-server deployment
 kubectl top nods
kubectl get hpa -w


HPA:
HPA stands for Horizontal Pod Autoscaler, and it's a feature in Kubernetes 
that automatically adjusts the number of replicas in a deployment or 
replica set based on observed CPU utilization or custom metrics. The 
goal of HPA is to ensure that an application has enough resources to 
handle varying workloads efficiently.

Here's a brief overview of how HPA works:

Metrics Collection: HPA continuously monitors the resource utilization metrics 
                  (such as CPU utilization) or custom metrics defined by the user.

Comparison with Target Value: The collected metrics are compared against a target 
                  value or range specified by the user. For example, you might 
                  set a target CPU utilization of 50%.

Scaling Decisions: Based on the comparison, the HPA makes scaling decisions. 
                  If the observed metrics are above the target, it means the 
                  application is under load, and HPA can increase the number 
                  of replicas. If the metrics are below the target, it may 
                  decrease the number of replicas to save resources.

Adjustment of Replicas: HPA communicates with the Kubernetes API server to adjust 
                        the number of replicas in the deployment or replica set.


Here's an example of how you might create an HPA for a deployment based on CPU utilization:

Deployment with HPA
==================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"
---
apiVersion: autoscaling/v2 
kind: HorizontalPodAutoscaler  
metadata:
  name: hpadeploymentautoscaler 
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment 
    name: hpadeployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
     name: memory
     target:
      type: Utilization
      averageUtilization: 40
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP

-# Create temp POD using below command interatively and increase the 
-# load on demo app by accessing the service.

kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh

run after that duplicate the mobaxtem

-# Access the service to increase the load.

while true; do wget -q -O- http://hpaclusterservice; done 


kubectl get hpa -w


-# Java Web Appliction With HPA
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unitedapp
spec:
  selector:
    matchLabels:
      app: unitedwebapp
  template:
    metadata:
      name: unitedwebapppod
      labels:
        app: unitedwebapp
    spec:
      containers:
      - name: unitedapp
        image: unitedcore/united-web-app
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 300m
            memory: 256Mi
          limits:
            cpu: 400m
            memory: 512Mi

Vertical Pod AutoScaling : 
Horizontal Pod AutoScaling  :
Cluster AutoScaling:

https://github.com/unitedcoresystems/springboot-mongo-app.git
-# Spring App & Mongod DB as POD without volumes
Stateless applications deployment:
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  

mongo database:
===============
  use StatefulSets as the choice kubernetes object for Stateful  
  application deployments   
  ReplicaSets/ReplicationController/deployment --- you cannot go back

Stateless
--- springapp.yml/
    springapp-deployment.yml   
    springapp-svc.yml  
    secret.yml 
    configMap.yml       

Stateful
--- mongo.yml/
    mongo-rs.yml  or  mongo-statefulset.yml / 
    mongo-svc.yml 
    persistenvolume.yml  
    persistenvolumeclaim.yml
    secret.yml 
    configMap.yml       
---

  mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: unitedcore/spring-boot-app
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080

# Corrected comment indentation
# appsvc---pod:8080

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017

==================================================================== 

29 Nov 2023
============

Kubernetes volumes:
==================
  

docker run -v data:/data/db -v data2:/data/db 

awsElasticBlockStore 
azureDisk
azureFile 
configMap 
emptyDir
gcePersistentDisk
gitRepo (deprecated) 
hostPath
nfs 
persistentVolumeClaim 
secret

Kind: Pod   
apiVersion: v1   
metadata:
  name: mysql    
spec:
  volumes:
  - name: mogodbhostvol
    hostPath
    nfs 
    awsElasticBlockStore 
    azureDisk
    azureFile 
    configMap 
    emptyDir 
    gcePersistentDisk
    gitRepo (deprecated) 
    persistentVolumeClaim 
    secret         



mongo-hostpath.yml  
==================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb-hp
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      containers:
      - name: mongodb-hpcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mogodbhostvol
          mountPath: /data/db
      volumes:
      - name: mogodbhostvol
        hostPath:
          path: /mongodata

Configuration of NFS Server
===========================

Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s 
repository index with that of the Internet through the following apt command as sudo:

mongo-nfs.yml  
==================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb-nfs
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      volumes:
      - name: mongodbvol
        nfs:
          server: 172.31.36.136 #IP of NFS Server
          path: /mnt/share
      containers:
      - name: mongodb-nfscontainer
        image: mongo #mysql
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbvol
          mountPath: /data/db # /var/lib/mysql


Installing Kops:

Configuration of NFS Server
===========================

Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s 
repository index with that of the Internet through the following apt command as sudo:
$sudo apt-get update
     the above command lets you install the latest available version of a software
     through the ubuntu repositories.
     Now, run the following command in order to install the NFS kernel server on 
     your system : 
     $sudo apt install nfs-kernel-server -y 
Step 2: Create the Export Directory [mount point]
      sudo mkdir -p /mnt/share/
      ## As we want all clients to access the directory, we will remove restrictive 
      permissions.
      sudo mkdir -p /mnt/share/
      sudo chown nobody:nogroup /mnt/share/
      sudo chmod 777 /mnt/share/
 step 3: Assgn server access to client(s) 
         through NFS export file

         sudo vi /etc/exports  

         # /mnt/share/  <clientIP or Clients
          CIDR>(rw,sync,no_subtree_check,no_root_squash)
          #Ex:
          /mnt/share/ *(rw,sync,no_subtree_check,no_root_squash)

          /mnt/share/ 10.0.2.2 (rw,sync,no_subtree_check,no_root_squash)

 Step 4: Export the shared directory
          $ sudo exportfs -a
          $ sudo systemctl restart nfs-kernel-server

 Step 5: Open firewall for the client (s) Port 2049.


Configuring the cient Machine
================================
Step 6: Install NFS common 
        Before installing the NFS common application 
        we need to update our system's repository
        index with that of the internet through the 
        following apt command as sudo:

        $sudo apt-get update -y && sudo apt-get install nfs-common -y 
        Install NFS agents on all worker nodes (Node1 and Node 11) 
        using the above command
        $sudo apt-get install nfs-common


Persistent volumes 
===================

deploy a kubernetes using Kops 


How to install kops on an ubuntu server: 


 SSH-Key Priv: SHA256:doZ+r/tLmacScdPJhiXf1AZmQqCVMnJaRu27jMWf7k0 kops@ip-172-31-36-13                                                                6

Pub: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCyX9lXpAFzWcFA73FSmhqBItADG5fbSrR1CHVSqv+qj3gJaT8dngB28e56tSlxJw9LsrhynKqkabLPcL7K3ujjt4VFItMEWWhS6oNSt/zMClzGEqTSlS3CWuAzGBLrtzKTmhsdbKUE4CapIwR9aSYIWvlet+QLMaphFt89+GVWQhincHXZqofBg7SfXG0ZtuummsHttIrPy/aJQzguN4niuMB7SuM3NSDlPC8gKFd4bj2XofOltWcYiMmmsRRD6Mp1TOH+BePpP4CjpL47lnL5p3jHcBuc3ZBFpTUfy/RxPKUQkim62Qyy7X4+/tE0uCHv8HUulQt5vP8nBQYzLg9IZOwvj93DfpMOTsiBiF3h21WUa3XOzJv7Vlv6VsBZXWxH2nvRH2AaEBu0pLyomdZbdLkZIhel9u5GtcE+yHnX2jwJpYQhdjUi/vAeLEafRms4j0W4FNc8SmHna3Mm91LeMl4Ht+7CTEX9hEqb0c+CUbdyc0v4HI3HurcN5tjRBK0= kops@ip-172-31-36-136


kops update cluster --name united2025.k8s.local --yes --admin   

kops export kubecfg $NAME --admin

Results:
 kOps has set your kubectl context to united2025.k8s.local
                               

Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster united2025.k8s.local
 * edit your node instance group: kops edit ig --name=united2025.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=united2025.k8s.local master-us-east-                                                                                                 1a


Persistent volumes:
===================

In Kubernetes, a Persistent Volume (PV) is a piece of storage in the 
cluster that has been provisioned by an administrator or dynamically 
provisioned using Storage Classes. It is a resource in the cluster 
that represents a real piece of storage, such as a physical disk in 
a node or a network-attached storage (NAS) device.


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.
        

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be 
    available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.
   
PVC: If pod requires access to storage(PV),it will get an access using PVC. PVC will be attached to PV.

PersistentVolume – the low level representation of a storage volume.
PersistentVolumeClaim – the binding between a Pod and PersistentVolume.
Pod – a running container that will consume a PersistentVolume.
StorageClass – allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce – the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany  – the volume can be mounted read-only by many nodes
ReadWriteMany – the volume can be mounted as read-write by many nodes = NFS 

   ebs / nfs-efs 

Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
Retain – When the claim(PVC) is deleted, the volume(PV) will exists.
Recycle – When the claim is deleted the volume remains but in a state where the data can be manually recovered.
Delete – The persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

kubectl exec -it mongodb-6hs5v  -- bash
mongosh -u devdb -p devdb@123 localhost:27017
show databases;
use users;
show collections; 
use users;
switched to db users
users> show collections;
users
users> db.users.find();



Demo add Persistence in draw.io:


https://www.mongodb.com/docs/manual/replication/
------------------------------------------------------


Demo the auto scallinggroup on AWS:
  Demo this on the CLI:


apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.32.113 #<nfs server Private ip>
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
pv-pvc-mongo-hostpath.yml  
=========================
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
pv-pvc-hostpath.yml   
==================
# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

pvc-nfs-pvc-mongo.yml 
====================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.32.113
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-nfs-pv1
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db
====================================


kubectl get pv

kubectl get pvc

kubectl get pods

kubectl get svc
kubectl get ed 

kubectl get po --show-labels
mongodbrs-nv2x4  1/1     Running   0  8m33s  app=mongodb

kubectl get svc --show-labels

kubectl edit svc mongosvc   

kubectl delete pod mongodb-nfs-bxljt --grace-period=0 --force

kubectl logs <pod_name> -c mongodb-nfscontainer


spec:
  terminationGracePeriodSeconds: 60 # Set to the desired timeout (in seconds)

StatefulSets:
  

spe:
  containers  
  volumes

https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/


kubectl exec -it monogodb-dzsl4 -- bash ---> the check the contect of the DB

