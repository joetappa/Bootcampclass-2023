Q1.Tell me about yourself and why you want to work in this DevOps role? 

Sample Answer: 
Over the years, I have gained a wide-ranging set of skills, qualities and attributes that, I believe, make me 
a competent, supportive, professional and flexible DevOps Engineer. I take pride in my work, I take my 
professional development seriously, and wherever I end up working, I always focus on how I can add value 
to the organization by providing secure and innovative solutions based on the needs of the business. In 
addition to possessing solid technical knowledge capabilities, I am also someone who has excellent 
communication, collaboration and decision making skills. That means, if you hire me within this DevOps 
role, you will not only be getting someone who always puts the needs of the team and the organization 
first, but you also get someone who is flexible and adaptable in their work so as to ensure you consistently 
achieve your commercial and financial objectives. 

Q2. What are the most important skills and qualities needed to work in this DevOps role? 

Sample Answer: 
To be effective when working in any DevOps role, you need three different type of skills: technical, soft and 
business. The TECHNICAL SKILLS needed include coding and scripting capabilities, infrastructure 
knowledge, cloud and testing skills, software security skills and also an understanding of major DevOps 
tools and resources. SOFT SKILLS required within DevOps include strong communication, interpersonal 
and collaboration capabilities, and also the ability to solve problems, be entirely flexible and adaptable in 
your work, and also the desire to maintain competence through continuous professional development. 
Finally, in terms of BUSINESS SKILLS, it’s imperative you have an understanding of how your work fits 
into the wider, strategic goals of the organization you are working for. 

Q3. Why do you want to work for our organization in this DevOps role? 

Sample Answer: 
For me, DevOps is a very exciting field to work in, providing of course, you choose the right organization 
to work for. Prior to applying for this DevOps role, I carried out lots of research into your organization to 
make sure it was somewhere I wanted to work for long-term, and also to make sure I was one hundred 
percent confident I could contribute positively to your goals and objectives. You are clearly an organization 
that has ambitious, exciting and diverse plans for the future, and I feel my knowledge, my skills and my 
experience will help you to achieve your goals. Finally, one of the influencing factors that made me really 
want to work for your organization, is the fact you employ lots of talented people. I want to work with a team 
of like-minded professionals who are all passionate about their work and who are also striving to achieve 
the same goal. For those reasons, I want to work here and nowhere else. 


Q4. Tell me about the hardest day you’ve had as a DevOps Engineer? 

Sample Answer: 
Perhaps the hardest day I have had as a DevOps Engineer was whilst working on a project for a client in 
a previous role. This was a cloud-based project and, despite having an initial set brief to work towards, 
the client continually changed the project specifications. Due to the client’s unfortunate haphazard 
approach to the project requirements, the team started to show signs of stress and frustration. I spoke to 
the team members and explained how important it was that we still provided a high level of service. 
Although it was frustrating to have to continually change our approach to the project, this was our 
opportunity to dig deep, maintain flexibility and also use patience and resilience to get through the project 
to a satisfactory conclusion. Although the project was very difficult to work on, we stuck together as a 
team, adapted as and when required and successfully completed everything the client wanted, on time 
and to the final requested specification. 

5. What are your strengths and weaknesses? 

Sample Answer: 
I would say my core strengths are my technical knowledge and expertise. I have built up lots of 
experience in various DevOps positions over the years, and I feel I can bring a wealth of knowledge and 
experience to your team. Other strengths include my communication and interpersonal skills. This means 
I can fit into a team quickly, and I will always be unselfish in my work and ensure the needs of the team 
and the organization always come first. Another strength of mine is my level of commercial awareness. I 
understand that, in order for your business to be successful, I have to excel in the position. In respect of 
my weakness, the only one I have is the fact I have trouble sometimes letting go of projects. I tend to get 
engrossed in projects and I become quite passionate about them. Having said that, I am learning to 
complete DevOps projects quickly and then move on to the next one, and I will always take onboard 
constructive feedback from my peers and managers in a positive way as I am someone who is keen to 
continually grow, learn and develop. 

Q6. You are giving a presentation to a number of non-technical company managers. They have no 
technical knowledge and they are struggling to understand your presentation. How would you 
adapt your presentation, so they all understand? 

Sample Answer: 
I would use visual explanations that they were all familiar with. For example, if the room was full of people 
who had a passion for cars, which lots of people do, I would use the construction of a car as the analogy 
when describing my technical solution or idea. For example, I might say: 
“If you try to imagine a car travelling along the road, the wheels enable the car to travel forward based on 
their shape. If you apply the same principle to my solution, you will see that object A is able to move forward 
because of object B.”, or something similar to that example that they can all relate to. I would also utilize 
drawings and diagrams, as the visual aspect is far easier to understand and follow. 

Q7. Where do you see yourself in the future? 

Sample Answer: 
I am looking long-term stable employment with an organization that is ambitious and that is also going 
places. On that basis, I plan to stay here for as long as you want me. You are a great organization with a 
very strong reputation in your industry. You are also a company that is innovative in its products and 
services, and you also understand that, in order for you to maintain your market share, you will have to 
continually change and adapt. That means you will continue to be an attractive organization to work for, 
so I would like to stay here long term if possible. 

Q8. How do you handle stress and pressure at work? 

Sample Answer: 
Pressure is all part of working in DevOps, and if I am being entirely truthful, I believe I actually work to the 
best of my ability when I am under pressure. Having said that, stress and pressure need to be managed 
effectively so that we can all work together to achieve the aims of the organization. To begin with, I believe 
in proper, effective planning of both time, tasks and projects. Planning helps significantly reduce stress 
within the workplace. Secondly, I believe in maintaining a positive mindset. If you are a positive, self- 
motivated and determined person, this rubs off on your work colleagues. Finally, I look after myself outside 
of work and I eat well, and whilst I am not a massive fitness fantastic, I often go walking and carry out 
physical exercise to clear my head and this also helps to reduce stress, too. 

Q9. What does DevOps mean to you and what is its purpose in our organization? 

Sample Answer: 
DevOps is all about facilitating a stronger alignment between IT and the business as a whole. This is 
achieved by working with IT developers to ensure effective coordination and integration between 
business operations, development and testing. DevOps, to me, is about improving organizational culture 
by implementing the right infrastructure and using the right tools to achieve the strategic goals of the 
business. 

Q10. Tell me how DevOps differs from Agile / SDLC? 

Sample Answer: 
I like to think of DevOps as a culture. That is, it is a culture of all teams collaborating together; both the 
development teams and operations teams. This collaboration means continuous development, feedback, 
improvements, and high-quality deliverables can be made swiftly and efficiently. Agile is an interactive 
framework, with qualities like those I have already mentioned such as the collaboration of all staff, customer 
feedback, on-going improvements and rapid releases of content. SDLC, or Software Development Life 
Cycle, focuses on ensuring the software is fully functional for the users by focussing on the design, 
maintenance and development workflows. Principally, Agile and SDLC are aimed at development teams, 
but DevOps ties everyone together collectively (both the development team and operations team) to create 
a more efficient workflow. Although there are often comparisons differentiating DevOps from Agile and 
SDLC, in reality, the Agile framework, can still be applied under the DevOps culture. 

Q11. What is an inode on Linux? 

Sample Answer: 
Essentially an inode is an administrative data source that stores a file's metadata i.e., the data needed to 
read the file. For example, the read, write permissions, the file type, and file size etc. is stored in the inode. 
Every file in Linux has an inode and each inode is identified with an integer number. 

Q12. In your opinion, which are the top DevOps tools and what are they used for? 

Sample Answer: Some of the tools below may not be familiar, just mention what best tools you know.
I have worked with many DevOps tools and the ones I think are best are: Kamatera is a cloud provider that 
is used for the deployment of cloud applications. Kamatera is really fast and supports the most popular 
applications such as CPanel, NextCloud, OpenVPN, Redis, WordPress, MySQL, node.js, & phpMyAdmin. 
Jenkins is another DevOps tool I recommend as it's an open-source tool that big companies like LinkedIn 
use as an automation server. By distributing work on multiple machines and platforms, it is great for 
automating, building, and the deployment in projects. The other tool I feel is one of the best is Git. Git is a 
version control system, that in my opinion, is the best for teams with different geographical locations. Its 
simplicity to learn and implement by teams makes this a top DevOps tool. You can add Kubernetes, Docker, Maven, SonaQube, Etc.

Q13. Tell me how you'd go about optimizing a database? 

Sample Answer: 
When optimizing a database, there are some essential areas I would cover right away. Firstly, I would 
index the database. I would specifically look to index "group by", "where", and "order by" columns. This will 
ensure unique records, speed up select query time, and allow for better sorting. I often see too many tables 
used in join statements. This can cause big optimization issues, so I always look to use less than a dozen 
joins for each query. Another common problem database problem is too many columns. As a rule of thumb, 
I try to keep this down to under 100 columns and will insert logical breaks. For instance, if a customer can 
have multiple email addresses, I may create a separate table for email addresses that reference back to 
the "customer_id". This can be a huge saver on CPU. Finally, I would ensure there are no null values (the 
absence of any value in a column). Null values can cause a database to not behave as intended when 
running queries. I would potentially define a default value if the database doesn't require a mandatory 
value. 

Q14. What is garbage collection programming? 

Sample Answer: 
Garbage collection is memory management in software. Often referred to as just the "Collector" it will 
attempt to reclaim memory/garbage allocated to objects that are no longer used by the program. There are 
essentially three forms of Garbage Collection; Mark and sweep, Copying collection, Reference counting. 
Garbage collection can save time for programmers by reducing the need for many functions, but at the 
same time, it can be performance-intensive as it has to run regularly to check object references and clean 
up. 

Q15. Talk me through the steps you would follow when checking physical memory on Linux? 

Sample Answer: 
The simplest way to check the physical memory on Linux server is to use the "free" command. I would do 
this by firstly opening the command line, and then I would type the command free followed by the byte 
format I would like the results displayed in such as and -g for gigabytes, or -m for megabytes. Within the 
results presented, I would look at the line starting with "Mem:" to see the physical memory. 

Q16. Explain how you copy a local file to HDFS? 

Sample Answer: 
The easiest way to do this would be to use the command line "hdfs dfs -copyToLocal" followed by the name 
of the source and the destination. If the system uses the legacy Hadoop fs then I would use the 
command hadoop fs -copyToLocal followed by the name of the source and the destination. 

Q17. Describe how you would make a background process run? 

Sample Answer: 
In Linux or Unix-like operating system, I would use the ampersand at the end of the command I use to run 
a job. This will tell the system to run the process in the background. For example, if I wanted to run the 
count programme I would enter the command count &. 

Q18. Assuming there are no cookies, how would you make your application work? 

Sample Answer: 
By utilizing sessions in PHP a web application can still function. Although sessions do normally use 
cookies, they are able to operate without them. This can be done by adding a session ID tag to any HTML 
forms, usually PHPSESSID. For any links in the HTML code, they will need to be modified to have a GET 
parameter added to the link. The GET parameter will also utilize the name of the PHPSESSID. The danger 
of doing this however is that the sessions can be then shared - if a person shares a link for example then 
they will be sharing their active session which someone could use to essentially steal their identity on that 
application and cause damage. 

Q19. Talk me through the process of what happens when someone accesses a URL via a web 
browser? 

Sample Answer: 
Firstly, once the URL/web address has been entered into the browser, the browser will go to the DNS 
server to find the IP address for the domain name. Secondly, the browser sends a HTTP request message 
to the server, essentially requesting the server to send a copy of the website to the browser. All data is 
sent via IP/TCP. Thirdly, the server then approves the request by returning a "200 OK" message and will 
then send across the website's files in data packets. Finally, the browser places these data packets 
together to render the site via HTML. 

Q20. What is the difference between CI and CD? 

Sample Answer: 
Continuous Integration (CI) and Continuous Delivery (CD) are both are different stages of modern 
development but are very different. CI is essentially the process of inserting code into a mainline codebase – nearly always using tools designed for this purpose whereas CD is actually for the processes needed 
after code has been inserted. For example, testing and deploying code. It is important to note that CD 
processes look different depending on the project and the team. There is no one tool for this. 

Q21. Tell me how you would measure network packet? 

Sample Answer: 
It's important to measure and analyze network packet loss as this is a common network performance issue. 
Essentially, packet loss if the number of data packets that fail to reach their destination, which I would 
measure by analyzing traffic data on both the sender and receiver ends. I would then determine if it was 
caused by software issues, router performance, or network congestion, as these are the most frequent 
factors for packet loss. I would measure network performance in other ways too by covering the following 
areas: latency, throughput, and bandwidth, jitters in addition to the four common areas that impact network 
performance: infrastructure, applications, network issues, and security issues. 

Q22. What is a distributed cache in Hadoop? Dont worry too much about this question, just read about Hadoop to know what it is and move on. 

Sample Answer: 
Distributed cache in Hadoop allows the copying of read-only files, jar files, or archives to a node before 
tasks are executed on that node. This way, it is possible to access files from all the nodes in a map and 
reduce the job. Upon successful completion of the job, the temporary/cache files are removed. 

Q23. There are a large number of requests due to high demand. How would you ensure your 
database works? 

Sample Answer: 
One way to improve an application's performance with high volumes of traffic is to introduce sharding. 
Sharding is the partitioning of data across multiple servers. This means each database stores less 
information and deals with fewer requests, thus improving performance. For example, a common practice 
is to separate customer IDs alphabetically into different shards, we could, for example, split customer IDs 
A-D, E-H, I-L, etc. into different shards. A potential downside to this method is the unbalance of data in 
each shard – we could see many more customers that have IDs between A-D than I-L, for example. 
Therefore, I would always analyze this and test a worst-case scenario before implementing any 
changes. Other areas I would consider are to ensure optimal indexing is in place and to ensure the latest 
version of MySQL is being used. Consideration of using a stronger CPU and SSD drive will also factor as 
demand increases. 

Q24. Why do you want to leave your current job? 

Sample Answer: 
I want to leave my job because I am looking for a fresh challenge with a company that has exciting and 
ambitious plans for the future, and one that will also use my skills and DevOps technical abilities to the full. 
My employer has been great, and we have achieved some fantastic things whilst I have been there, but I 
am now ready for a new challenge as a DevOps Engineer, and I would like that challenge to be with your 
company. 

Q25. What are your salary expectations in this DevOps Engineer role? 

Sample Answer: 
I have carried out some research in relation to the average salary for this DevOps position, and the general 
range is between $120,000 and $140,000. Whilst I do personally feel I am worth the higher end of the salary 
scale, I understand and appreciate I need to prove to you my worth. On that basis, I would be comfortable 
with a salary of $135,000. 
NOTE: Please conduct your own research into the general salary range for a DevOps Engineer in your 
area. 

Q26. That’s the end of your DevOps interview, do you have any questions for the panel? 

Sample Answer: 
Thank you. Yes, I do: 

Q. What are the most difficult DevOps challenges you have been facing from an organizational perspective 
over the last 12 months? 

Q. If I am successful, what would you need me to concentrate on immediately within the first few weeks of 
starting? 

Q. Can you tell me what the culture is like within the organization? 

Q. Do you have any exciting or new plans for the organization over the forthcoming 12 months that I would 
be involved in if I am successful? 

How to Use These Interview Questions and Answers: 
These interview questions and answers are intended to guide you in your preparation for your job 
interview. These questions have been picked by a team because we believe 
that they are the best representative of what you will face in your interview. 
The sample answers in this resource are collated from years of experience and research in the 
recruitment sector. The answers confidently display the appropriate qualities and competencies 
that the interviewer expects from successful candidates. 
Read the sample answers carefully, and take note of what skills and competencies they 
demonstrate. You might notice that, when the question asks for examples, the answer uses the 
STAR method to construct the response: 
Situation. Start off your response to the interview question by explaining what the ‘situation’ was 
and who was involved. 
Task. Once you have detailed the situation, explain what the ‘task’ was, or what needed to be 
done. 
Action. Now explain what ‘action’ you took, and what action others took. Also explain why you 
took this particular course of action. 
Result. Explain to the panel what you would do differently if the same situation arose again. It is 
good to be reflective at the end of your responses. This demonstrates a level of maturity and it 
will also show the panel that you are willing to learn from every experience. 
In order to get the best possible results, apply this system to your own examples and experiences 
in working life. These sample answers are intended to inspire you to create your own responses 
to the questions. 




# 35 General Interview Questions 

- **Question 1: Tell me about yourself?**
ANSWER:
- **Question 2: What are your greatest strengths?** 
ANSWER:
- **Question 3: What are your greatest weaknesses?**
ANSWER:
- **Question 4: Tell me about the greatest mistake you ever made in your career.**
ANSWER:
- **Question 5: Why are you leaving (or did you leave) this most recent position?**
ANSWER:
- **Question 6: Why should I hire you?**
ANSWER:
- **Question 7: Aren't you overqualified for this position?**
ANSWER:
- **Question 8: Where do you see yourself five years from now?**
ANSWER:
- **Question 9: Describe your ideal company, location and job.**
ANSWER:
- **Question 10: Why do you want to work at our company?**
ANSWER:
- **Question 11: What are your career options right now?**
ANSWER:
- **Question 12: Why have you been out of work so long?**
ANSWER:
- **Question 13: Tell me honestly about the strong points and weak points of your boss (company, management team, etc.)**
ANSWER:
- **Question 14: Tell me about a situation when your work was criticized**
ANSWER:
- **Question 15: How do you feel about reporting to a younger person (minority, woman, etc)?.**
ANSWER:
- **Question 16: Could you have done better in your last job?**
ANSWER:
- **Question 17:Can you work under pressure?**
ANSWER:
- **Question 18: Who has inspired you in your life and why?**
ANSWER:
- **Question 19: What was the toughest decision you ever had to make?**
ANSWER:
- **Question 20: Tell me about the most boring job you've ever had.**
ANSWER:
- **Question 21: Have you been absent from work more than a few days in any previous position?**
ANSWER:
- **Question 22: What changes would you expect to make if you came on board?**
ANSWER:
- **Question 23: I'm concerned that you don't have college degree/certification as much experience as we'd like in this filed/area.**
ANSWER:
- **Question 24: How do you feel about working nights and weekends?**
ANSWER:
- **Question 25: Are you willing to relocate or travel?**
ANSWER:
- **Question 26: How could you have improved your career progress?**
ANSWER:
- **Question 27: Give me an example of your creativity (analytical skill...managing ability, etc.)**
ANSWER:
- **Question 28: What's the most difficult part of being a (your job title)?**
ANSWER:
- **Question 29: What was the toughest challenge you've ever faced?**
ANSWER:
- **Question 30: Where do you see yourself in 5 years?**
ANSWER:
- **Question 31: What was the toughest part of your last job?**
ANSWER:
- **Question 32: Looking back on your last position, what are some of your best experience?**
ANSWER:
- **Question 33: How do you define success...and how do you measure up to your own
definition?**
ANSWER:
- **Question 34: Tell me something negative you've heard about our company. (A variation on
“What do you know about our company?”)**
ANSWER:
- **Question 35: Why should I hire you from the outside when I could promote someone from within?**

- DevOps Interview Questions with Answers  
 
1. What is Source Code Management?  
It is a process through which we can store and manage any code. 
Developers write code, Testers write test cases and DevOps 
engineers write scripts. This code, we can store and manage in 
Source Code Management. Different teams can store code 
simultaneously. It saves all changes separately. We can retrieve this 
code at any point of time.  

2. What are the Advantages of Source Code Management?  
. Helps in Achieving teamwork  
. Can work on different features simultaneously  
. Acts like pipeline b/w offshore & onshore teams  
. Track changes (Minute level)  
. Different people from the same team, as well as different teams, can 
store code simultaneously (Save all changes separately)  

3. Available Source Code Management tools in the market?  
There are so many Source Code Management tools available in the 
market. Those are  
. Git  
. SVN  
. Perforce  
. Clear case  
Out of all these tools, Git is the most advanced tool in the market 
where we are getting so many advantages compared to other Source 
Code Management tools.  

4. What is Git?  
Git is one of the Source Code Management tools where we can store 
any type of code. Git is the most advanced tool in the market now. We 
also call Git is version control system because every update stored as 
a new version. At any point of time, we can get any previous version. 
We can go back to previous versions. Every version will have a 
unique number. That number we call commit-ID. By using this commit 
ID, we can track each change i.e. who did what at what time. For 
every version, it takes incremental backup instead of taking the whole 
backup. That’s why Git occupies less space. Since it is occupying less 
space, it is very fast.  

5 What are the advantages of Git?  
Speed:-  
Git stores every update in the form of versions. For every version, it 
takes incremental backup instead of taking the whole backup. Since it 
is taking less space, Git is very fast. That incremental backup we call 
“Snapshot”  
.Parallel branching:-  
We can create any number of branches as per our requirement. No 
need to take prior permission from any one, unlike other Source 
Code Management tools. Branching is for parallel development. Git 
branches allow us to work simultaneously on multiple features.  
.Fully Distributed:-  
A backup copy is available in multiple locations in each and 
everyone’s server instead of keeping in one central location, unlike 
other Source Code  
Management tools. So even if we lose data from one server, we can 
recover it easily. That’s why we call GIT as DVCS (Distributed Version 
Control System)  
Follow me for more DevOps Documents: Shivam Agnihotri 

5. What are the stages in Git?  
There are total of 4 stages in Git  
1. Workspace:-  
It is the place where we can create files physically and modify. Being 
a Git user, we work in this work space.  
2. Staging area/Indexing area:-  
In this area, Git takes a snapshot for every version. It is a buffer zone 
between workspace and local repository. We can’t see this region 
because it is virtual.  
3. Local repository:-  
It is the place where Git stores all commit locally. It is a hidden 
directory so that no one can delete it accidentally. Every commit will 
have unique commit ID.  
4. Central repository:-  
It is the place where Git stores all commit centrally. It belongs to 
everyone who is working in your project. Git Hub is one of the central 
repositories. Used for storing the code and sharing the code to others 
in the team.  

6 What is the common branching strategy in Git?  
• Product is the same, so one repo. But different features.  
• Each feature has one separate branch  
• Finally, merge (code) all branches  
• For Parallel development  
• Can create any no of branches  
• Can create one branch on the basis of another branch  
• Changes are personal to that particular branch  
• Can put files only in branches (not in repo directly)  
• The default branch is “Master”  
• Files created in a workspace will be visible in any of the branch 
workspaces until you commit. Once you commit, then that file 
belongs to that particular branch.  

7. How many types of repositories available in Git?  
There are two types of repositories available in Git  
Bare Repositories (Central)  
These repositories are only for Storing & Sharing the code  
All central repositories are bare repositories  
Non – Bare Repositories (Local)  
In these repositories, we can modify the files  
All local /user repositories are Bare Repositories  

8. Can you elaborate commit in Git?  
• Storing file permanently in the local repository we call commit.  
• For every commit, we get one commit ID  
• It contains 40 long Alpha-numeric characters  
• It uses the concept “Check some” (It’s a tool in Linux, generates 
binary value equal to the data present in file)  
• Even if you change one dot, Commit-ID will get changed  
• Helps in tracking the changes  

9. What do you mean by “Snapshot” in Git?  
• It is a backup copy for each version git stores in a repository.  
• Snapshot is an incremental backup copy (only backup for new 
changes)  
• Snapshot represents some data of particular time so that, we can 
get data of particular time by taking that particular snapshot  
• This snapshot will be taken in Staging area in Git which is 
present between Git workspace and Git local repository.  

10. What is GitHub?  
Git hub is central git repository where we can store code centrally. 
Git hub belongs to Microsoft Company. We can create any number of 
repositories in Git hub. All public repositories are free and can be 
accessible by everyone.  
Private repositories are not free and can restrict public access for 
security. We can copy the repository from one account to other 
accounts also. This process we call as “Fork”. In this repository also 
we can create branches. The default branch is “Master”  

11. What is Git merge?  
By default, we get one branch in git local repository called “Master”. 
We can create any no of branches for parallel development. We write 
code for each feature in each branch so that development happens 
separately. Finally, we merge code off all branches in to Master and 
push to central repository. We can merge code to any other branch as 
well. But merging code into master is standard practice that being 
followed widely. Sometimes, while merging, conflict occurs. When 
same file is in different branches with different code, when try to 
merge those branches, conflict occurs. We need to resolve that 
conflict manually by rearranging the code.  

12. What is Git stash?  
We create multiple branches to work simultaneously on multiple 
features. But to work on multiple tasks simultaneously in one branch 
(i.e. on one feature), we use git stash. Stash is a temporary repository 
where we can store our content and bring it back whenever we want 
to continue with our work with that stored content. It removes content 
inside file from working directory and puts in stashing store and gives 
clean working directory so that we can start new work freshly. Later 
on you can bring back that stashed items to working directory and 
can resume your work on that file. Git stash applicable to modified 
files. Not new files. Once we finish our work, we can remove all 
stashed items form stash repository.  

13. What is Git Reset?  
Git Reset command is used to remove changes form staging area. This 
is bringing back file form staging area to work directory. We use this 
command before commit. Often we go with git add accidentally. In 
this case if we commit, that file will be committed. Once you commit, 
commit ID will be generated and it will be in the knowledge of 
everyone. So to avoid this one, we use Git reset.  
If you add “–hard” flag to git reset command, in one go, file will be 
removed from staging area as well as working directory. We 
generally go with this one if we fell that something wrong in the file 
itself.  

15. What is Git Revert?  
Git Revert command is used to remove changes from all 3 stages 
(work directory, staging area and local repository). We use this 
command after commit. Sometimes, we commit accidentally and later 
on we realize that we shouldn’t have done that. For this we use Git 
revert. This operation will generate new commit ID with some 
meaningful message to ignore previous commit where mistake is 
there. But, here we can’t completely eliminate the commit where 
mistake is there. Because Git tracks each and every change.  
Follow me for more DevOps Documents: Shivam Agnihotri 

16. Difference between Git pull and Git clone?  
We use these two commands to get changes from central repository. 
For the first time if you want whole central repository in your local 
server, we use git clone. It brings entire repository to your local 
server. Next time onwards you might want only changes instead of 
whole repository. In this case, we use Git pull.  
Git clone is to get whole copy of central repository  
Git pull is to get only new changes from central repository 
(Incremental data)  

17. What is the difference between Git pull and Fetch?  
We use Git pull command to get changes from central repository. In 
this operation, internally two commands will get executed. One is Git 
fetch and another one is Git merge.  
Git fetch means, only bringing changes from central repo to local 
repo. But these changes will not be integrated to local repo which is 
there in your server. Git merge means, merging changes to your local 
repository which is there in your server. Then only you can see these 
changes.  
So Git pull is the combination of Git pull and Git merge.  

18. What is the difference between Git merge and rebase?  
We often use these commands to merge code in multiple branches. 
Both are almost same but few differences. When you run Git merge, 
one new merge commit will be generated which is having the history 
of both development branches. It preserves the history of both 
branches. By seeing this merge commit, everyone will come to know 
that we merged two branches. If you do Git rebase, commits in new 
branch will be applied on top of base branch tip. There won’t be any 
merge commit here. It appears that you started working in one single 
branch form the beginning. This operation will not preserves the 
history of new branch.  

19. What is Git Bisect?  
Git Bisect we use to pick bad commit out of all good commits. Often 
developers do some mistakes. For them it is very difficult to pick that 
commit where mistake is there. They go with building all commits one 
by one to pick bad commit. But Git bisect made their lives easy. Git 
bisect divides all commits equally in to two parts (bisecting equally). 
Now instead of building each commit, they go with building both 
parts. Where ever bad commit is there, that part build will be failed. 
We do operation many times till we get bad commit. So Git bisect 
allows you to find a bad commit out of good commits. You don’t have 
to trace down the bad commit by hand; git-bisect will do that for you.  

20. What is Git squash?  
To move multiple commits into its parent so that you end up with one 
commit. If you repeat this process multiple times, you can reduce “n” 
number of commits to a single one. Finally we will end up with only 
one parent commit. We use this operation just to reduce number of 
commits.  

21. What is Git hooks?  
We often call this as web hooks as well. By default we get some 
configuration files when you install git. These files we use to set some 
permissions and notification purpose. We have different types of 
hooks (pre commit hooks & post commit hooks)  
Pre-commit hooks:- Sometimes you would want every member in 
your team to follow certain pattern while giving commit message. 
Then only it should allow them to commit. These type of restrictions 
we call pre-commit hooks.  
Post-commit hooks:- Sometimes, being a manager you would want an 
email notification regarding every commit occurs in a central 
repository. This kind of things we call post-commit hooks.  
In simple terms, hooks are nothing but scripts to put some 
restrictions.  

22. What is Git cherry-pick?  
When you go with git merge, all commits which are there in new 
development branch will be merged into current branch where you 
are. But sometimes, requirement will be in such that you would want 
to get only one commit form development branch instead of merging 
all commits. In this case we go with git cherry-pick. Git cherry-pick 
will pick only one commit whatever you select and merges with 
commits which are there in your current branch. So picking particular 
commit and merging into your current branch we call git cherry-pick.  

23. What is the difference between Git and SVN?  
SVN:-  
It is centralized version control system (CVCS) where back up copy 
will be placed in only one central repository.  
There is no branching strategy in SVN. You can’t create branches. So 
no parallel development.  
There is no local repository. So can’t save anything locally. Every time 
after writing code you need to push that code to central repository 
immediately to save changes.  
Git:-  
It is a Distributed version control system where back up copy is 
available in everyone’s machine’s local repository as well as a 
central repository. We can create any no of branches as we want. 
So we can go in parallel development simultaneously.  
Every Git repository will have its own local repository. So we can save 
changes locally. At the end of our work finally, we can push code to a 
central repository.  

25. What is the commit message in Git?  
Every time we commit, while committing, we have to give commit 
message just to identify each commit. We can’t remember to commit 
numbers because they contain 40 long alphanumeric characters. So, 
to remember commits easily, we give commit message. The format of 
commit message differs from company to company and individual to 
individual.  
We have one more way to identify commits. That is giving “Tags”. 
Tag is a kind of meaningful name to a particular commit. Instead of 
referring to commit ID, we can refer to tags. Internally tag will refer 
to respective commit ID. These are the ways to get a particular 
commit easily.  

26. What is Configuration Management?  
It is a method through we automate admin tasks. Each and every 
minute details of a system, we call configuration details. If we do any 
change here means we are changing the configuration of a machine. 
That means we are managing the configuration of the machine. 
System administrators used to manage the configuration of machine 
through manually. DevOps engineers are managing this configuration 
through automated way by using some tools which are available in 
the market. That’s why we call these tools as configuration 
management tools.  
Follow me for more DevOps Documents: Shivam Agnihotri 

27. What is IAC?  
IAC means Infrastructure As Code. It is the process through which we 
automate all admin tasks. Here we write code in Ruby script in chef. 
When you apply this code, automatically code will be converted into 
Infrastructure. So here we are getting so many advantages in writing 
the code. Those are  
1. Code is Testable (Testing code is easy compare to Infrastructure)  
2. Code is Repeatable (Can re-use the same code again and again)  
3. Code is Versionable (Can store in versions so that can get any 
previous versions at any time)  

28. What do you mean by IT Infrastructure??  
IT Infrastructure is a composite of the following things  
• Software  
• Network  
• People  
• Process  

29. What are the problems that system admins used to face earlier 
when there were no configuration management tools?  
1. Managing users & Groups is big hectic thing (create users and 
groups, delete, edit……)  
2. Dealing with packages (Installing, Upgrading & Uninstalling)  
3. Taking backups on regular basis manually  
4. Deploying all kinds of applications in servers  
5. Configure services (Starting, stopping and restarting services)  
These are some problems that system administrators used to face 
earlier in their manual process of managing configuration of any 
machine.  

30. Why should we go with Configuration Management Tool?  
1. By using the Configuration Management Tool, we can automate 
almost each and every admin task.  
2. We can increase uptime so that can provide maximum user 
satisfaction.  
3. Improve the performance of systems.  
4. Ensure compliance  
5. Prevent errors as tools won’t do any errors  
6. Reduce cost (Buy tool once and use 24/7)  

31. How this Configuration Management Tool works?  
Whatever system admins (Linux/windows) used to do manually, now 
we are automating all those tasks by using any Configuration 
Management Tool. We can use this tool whether your servers are in 
on-premises or in the cloud. It turns your code into infrastructure. So 
your code is versionable, repeatable and testable. You only need to 
tell what the desired configuration should be, not how to achieve it. 
Through automation, we get our desired state of server. This is unique 
feature of Configuration Management Tool.  

32. What is the architecture of Chef?  
Chef is an administration tool. In this we have total 3 stages.  
1. Chef Workstation (It is the place where we write code)  
2. Chef Server (It is the place where we store code)  
3. Chef Node (It is the place where we apply code)  
We need to establish communication among workstation, server 
and nodes. You can have any no of nodes. There is no limit. Chef can 
manage any no of nodes effectively.  

33. Components of Chef?  
Chef Workstation: Where you write the code  
Chef Server: Where you upload the code  
Chef Node: Where you apply the code  
Knife: Tool to establish communication among workstation, server & 
node.  
Chef-client: Tool runs on every chef node to pull code from chef 
server  
Ohai: Maintains current state information of chef node (System 
Discovery  
Tool)  
Idempotency: Tracking the state of system resources to ensure that 
the changes should not re-apply repeatedly.  
Chef Supermarket: Where you get custom code  

34. How does Chef Works?  
We need to install chef package in workstation, server and nodes. We 
create cookbook in workstation. Inside cookbook, there will be a 
default recipe where you write code in ruby script. You can create 
any no of recipes. There is no limit. After writing code in recipe, we 
upload whole cookbook to chef server. Chef server acts as central 
hub storing code. Then, we need to add this cookbook’s recipe to 
nodes run-list. Chef-client tool will be there in each and every chef 
node. It runs frequently. Chef-client comes to chef server and take 
that code and applies that code in node. This is how code will be 
converted into infrastructure.  

35. What is Idempotency?  
It is unique feature in all configuration management tools. It ensures 
that changes should not re-apply repeatedly. Once chef-client 
converted code into Infrastructure, then even chef-client runs again, it 
will not take any action. It won’t do the same task again and again. If 
any new changes are there in that code, then only chef-client is going 
to take action. So it doesn’t make any difference ever if you run chef
client any no of times. So tracking the system details to not to reapply 
changes again and again, we call Idempotency.  

36. What is Ohai and how does it works??  
Ohai we call “System Discovery Tool”. It stores system information. It 
captures each and every minute details of system and updates it then 
and there if any new changes are there. Whenever chef-client 
converts code in infrastructure in node, immediately Ohai store will 
be updated. Next time onwards, before chef-client runs, it verifies in 
Ohai store to know about current state of information. So chef-client 
will come to know the current state of server. Then chef-client acts 
accordingly. If new changes are there, then only it will take action. If 
there are no new changes, then it won’t take any action. Ohai tool 
helps in achieving this.  

37. How many types of chef server?  
Total there are 3 ways through which we can manage chef server.  
1. 
Directly we can take chef server from Chef Company itself. In 
this case, everything will be managed by Chef Company. You will get 
support from chef. This type of server we call Managed/Hosted chef. 
This is completely Graphical User Interface (GUI). It’s not free. We 
need to pay to Chef Company after exceeding free tier limit.  
2. 
We can launch one server and we need to install chef server 
package. It is completely free package. It’s GUI.  
3. 
We can launch one server and we need to install chef server 
package. It is completely free package. It’s CLI (Command Line 
Interface).  

38. What is there inside cookbook??  
Below mentioned files and folders will be there inside cookbook 
when you first create it  
Chefignore: like .gitignore (to ignore files and folders)  
Kitchen.yml: for testing of cookbook  
Metadata.rb: name, author, version…. etc of cookbook  
Readme.md: information about usage of cookbook  
Recipe: It is a file where you write code  
Spec: for unit test  
Test: for integration test  

39. What is Attributes concept in chef?  
Sometimes we might need to deploy web applications to in nodes and 
for that we need to know some host specific details of each server like 
IP Address, Host name ….. etc. Because we need to mention that in 
configuration files of each server. These files we call as Configuration 
files. This information will be vary from system to system. These host 
specific details that we mention in Configuration files,we call 
“Attributes”. Chef-client tool gathers these Attributes from Ohai store 
and puts in configuration files. Instead of hard coding these attributes, 
we mention as variables so that every time, file will be updated with 
latest details of their respective nodes.  

40. What is Run-list in Chef?  
This is an ordered list of recipes that we are going to apply to nodes. 
We mention all recipes in cookbook and then we upload that 
cookbook to chef server. Then, we attach all recipes to nodes run-list 
in sequence order. When chef-client runs, it applies all recipes to 
nodes in the same order whatever the order you mention in run-list. 
Because sometimes order is important especially when we deal with 
dependent recipes.  

41. What is bootstrap?  
It is the process of adding chef node to chef server or we can call, 
bringing any machine into chef environment. In this bootstrapping 
process total three action will be performed automatically.  
1. Node gets connected to chef server.  
2. Chef server will install chef package in chef node.  
3. Cookbooks will be applied to chef node.  
It is only one time effort. As and when we purchase any new 
machine in company, immediately we add that server to chef 
server. At a time, we can bootstrap one machine. We can’t 
bootstrap multiple machines at a time.  

42. What is the workflow of Chef?  
We connect chef workstation, chef server and chef node with each 
other. After that, we create cookbook in chef workstation and inside 
that cookbook, we write code in recipe w.r.t. the infrastructure to be 
created. Then we upload entire cookbook to chef server and attach 
that cookbook’s recipe to nodes runlist. Now we automate chef-client 
which will be there in all chef nodes. Chefclient runs frequently 
towards chef server for new code. So chef-client will get that code 
from server and finally applies to chef node. This is how, code is 
converted into infrastructure. If no changes are there in code, even if 
chefclient runs any no of time, it won’t take any action until it finds 
some changes in code. This is what we call Idempotency.  
Follow me for more DevOps Documents: Shivam Agnihotri 

43. How does we connect Chef Workstation to Chef Server?  
First we download started kit from chef server. This will be 
downloaded in the form of zip file. If we extract this zip file, we will 
get chef-repo folder. This chefrepo folder we need to place in chef 
workstation. Inside chef-repo folder, we can see total three folders. 
They are .chef, cookbooks and roles. Out of these three, .chef folder 
is responsible to establish communication between chef server and 
chef workstation. Because, inside .chef folder, we can see two files. 
They are knife.rb and organization.pem. Inside kinfe.rb, there will be 
the url (address) of chef server. Because of this url, communication 
will be established between chef server and chef workstation. This is 
how we connect Chef Workstation to Chef Server.  

44. How does the chef-client runs automatically?  
By default, chef-client runs manually. So we need to automate this 
manually. For this, we use “cron tool” which is the default tool in all 
Linux machines use to schedule tasks to be executed automatically at 
frequent intervals. So in this “crontab” file, we give chef-client 
command and we need to set the timing as per our requirement. Then 
onwards chef-client runs automatically after every frequent intervals. 
It is only one time effort. When we purchase any new server in 
company, along with bootstrap, we automate chef-client then and 
there.  

45. What is chef supermarket?  
Chef supermarket is the place where we get custom cookbooks. 
Every time we need not to create cookbooks and need not to write 
code from scratch. We can go with custom cookbooks which are 
available in chef supermarket being provided by chef organization 
and community. We can download these cookbooks and modify as 
per our needs. We get almost each and every cookbook from chef 
supermarket. They are safe to use.  

46. What is wrapper cookbook?  
Either we can download those chef supermarket cookbooks or 
without downloading, we can call these supermarket cookbooks 
during run time so that every time we get updates automatically for 
that cookbook if any new updates are there. Here, we use our own 
cookbook to call chef supermarket cookbook. This process of calling 
cookbook by using another cookbook, we call wrapper cookbook. 
Especially, we use this concept to automate chef-client.  

47. What is “roles” in chef?  
Roles are nothing but a Custom run-list. We create role & upload to 
chef server & assign them to nodes. If we have so many nodes, need 
to add cookbook to run-list of all those nodes, it is very difficult to 
attach to all nodes run-list. So, we create role & attach that role to all 
those nodes once. Next time onwards, add cookbook to that role. 
Automatically, that cookbook will be attached to all those nodes. So 
role is one time effort. Instead of adding cookbooks to each & every 
node’s run-list always, just create a role & attach that role to nodes. 
When we add cookbook to that role, it will be automatically applied to 
all nodes those assigned with that role.  

48. What is include_recipe in chef?  
By default, we can call one recipe at a time in one cookbook. But if 
you want to call multiple recipes from same cookbook, we use 
include_recipe concept. Here, we take default recipe and we mention 
all recipes to be called in this default recipe in an order. If we call 
default recipe, automatically default recipe will call all other recipes 
which are there inside default recipe. By using one recipe, we can 
call any no of recipes. This process of calling one recipe by using 
other recipe, we call as include_recipe. Here condition is we can call 
recipes from same cookbook, but not from different cookbooks.  

49. How to deploy a web server by using chef?  
package ‘httpd’ do  
action :install 
end  
file 
‘/var/www/html/index.html’ 
do 
content 
‘Hello 
Dear 
Students!!’ action :create end  
service ‘httpd’ do  
action [ :enable, :start ] 
end  

50. How to write ruby code to create file, directory?  
file ‘/myfile’ do content 
‘This is my second file’ 
action :create owner ‘root’ 
group ‘root’ end  
directory ‘/mydir’ 
do action :create 
owner 
‘root’ 
group ‘root’ end  

51. How to write ruby code to create user, group and install 
package?  
user ‘user1’ 
do 
action: 
create end  
group ‘group1’ 
do action 
:create 
members 
‘user1’ append 
true end  
package ‘httpd’ do  
action: install 
end  

52. What is container?  
The container is like a virtual machine in which we can deploy any 
type of applications, soft wares and libraries. It’s a light weight virtual 
machine which uses OS in the form of image, which is having less in 
size compare to traditional VMware and oracle virtual box OS images. 
Container word has been taken from shipping containers. It has 
everything to run an application.  

53. What is virtualization?  
Logically dividing big machine into multiple virtual machines so that 
each virtual machine acts as new server and we can deploy any kind 
of applications in it. For this first we install any virtualization software 
on top of base OS. This virtualization software will divide base 
machine resources in to logical components. In a simple terms, 
logically dividing one machine into multiple machines we call 
virtualization.  

54. What is Docker?  
Docker is a tool by using which, we create containers in less time. 
Docker uses light weight OS in the form of docker images that we will 
get from docker hub. Docker is open source now. It became so 
popular because of its unique virtualization concept called 
“Containerization” which is not there in other tools. We can use 
docker in both windows and Linux machines.  

55. What do you mean by docker image?  
Docker image is light weight OS provided by docker company. We 
can get any type of docker image form docker hub. We use these 
docker images to create docker containers. This docker images may 
contain only OS or OS + other soft wares as well. Each software in 
docker image, will be stored in the form of layer. Advantage of using 
docker images is, we can replicate the same environment any no of 
times.  

56. What are the ways through which we can create docker 
images?  
There are three ways through which we can create docker images.  
1. 
2. 
We can take any type of docker image directly from docker hub 
being provided by docker company and docker community.  
We can create our own docker images form our own docker 
containers. I.e. first we create container form base docker image 
taken form docker hub and then by going inside container, we install 
all required soft wares and then create docker image from our own 
docker container.  
3. 
We can create docker image form docker file. It is the most 
preferred way of creating docker images.  

57. What is docker file and why do we use it?  
It is a just normal text file with instructions in it to build docker image. 
It is the automated way of creating docker images. Once you build 
docker image, automatically docker file will be created. In this file, 
we mention required OS image and all required soft wares in the form 
of instructions. Once we build docker file, back end, docker container 
will be created and then docker image will be crated from that 
container and that container will be destroyed automatically.  

58. Difference between docker and VM Ware?  
VM Ware uses complete OS which contains GBs in size. But docker 
image size is MBs only. So it takes less size. That’s why it takes less 
base machine resources. This docker image is compressed version of 
OS. The second advantage of docker is, there is no pre-allocation of 
RAM. During run time, it takes RAM as pre requirement from base 
machine and one’s job is done, it release RAM. But in VM Ware, pre
allocation of RAM is there and it blocked whether it uses or not. So 
need more RAM for base machine if you want to use VM Ware unlike 
Docker.  
Follow me for more DevOps Documents: Shivam Agnihotri 

59. What is OS-Lever Virtualization?  
It is the unique feature of Docker which is not available in other 
virtualization soft wares. Docker takes most of UNIX features form host 
machine OS and it only takes extra layers of required OS in the form 
of docker image. So docker image contains only extra layers of 
required OS. For core UNIX kernel, it depends upon host OS, why 
because UNIX kernel is same in any of the UNIX and Linux flavors. In a 
simple terms, docker takes host OS virtually. That’s why we call this 
concept as OS-Lever Virtualization.  

60. What is Layered file system/Union file system?  
Inside docker container, wheat ever we do, that forms as a new layer. 
For instance, creating files, directories, installing packages etc. This 
is what we call as layered file system. Each layer takes less space. We 
can create docker image form this container. In that docker image 
also we get all these layers and forms unity. That’s why we also call 
Union File System. If we create container out of docker image, you 
can able to see all those files, directories and packages. This is what 
replication of same environment.  

61. What are the benefits of Docker?  
• Containerization (OS level virtualization) (No need guest OS)  
• No pre-allocation of RAM  
• Can replicate same environment  
• Less cost  
• Less weight (MB’s in size)  
• Fast to fire up  
• Can run on physical/virtual/cloud  
• Can re-use (same image)  
• Can create containers in less time  

62. List of Docker components?  
Docker image: – Contains OS (very small) (almost negligible) + soft 
wares Docker Container: – Container like a machine which is 
created from Docker image.  
Docker file: – Describes steps to create a docker image.  
Docker hub/registry: – Stores all docker images publicly.  
Docker daemon: – Docker service runs at back end  
Above five components we call as Docker components  

63. What is Docker workflow?  
First we create Docker file by mentioning instructions to build docker 
image. Form this Docker image, we are going to create Docker 
container. This Docker image we can push to docker hub as well. This 
image can be pulled by others to create docker containers. We can 
create docker images from docker containers. Like this we can create 
Docker images form either docker file or docker containers. We can 
create docker containers from docker images. This is the work flow of 
docker.  

64. Sample Docker file instructions?  
FROM ubuntu  
WORKDIR /tmp  
RUN echo “Hello” > /tmp/testfile  
ENV myname user1  
COPY testfile1 /tmp  
ADD test.tar.gz /tmp  

65. What is the importance of volumes in Docker?  
• Volume is a directory inside your container  
• First declare directory as a volume and then share volume  
• Even if we stop container, still we can access volume  
• Volume will be created in one container  
• You can share one volume across any no of containers  
• Volume will not be included when you update an image  
• Map volumes in two ways  
• Share host – container  
• Share container – container  

66. What do you mean by port mapping in Docker?  
Suppose if you want to make any container as web server by installing 
web package in it, you need to provide containers IP address to 
public in order to access website which is running inside docker 
container. But Docker containers don’t have an IP address. So, to 
address this issue, we have a concept called Docker port mapping. 
We map host port with container port and customers use public IP of 
host machine. Then their request will be routed from host port to 
container’s port and will be loaded web page which is running inside 
docker container. This is how we can access website which is running 
inside container through port mapping.  

67. What is Registry server in Docker?  
Registry server is our own docker hub created to store private docker 
images instead of storing in public Docker hub. Registry server is one 
of the docker containers. We create this Registry server from 
“registry” image, especially provided by docker to create private 
docker hub. We can store any no of private docker images in this 
Registry server. We can give access to others, so that, they also can 
store their docker images whomever you provide access. Whenever 
we want, we can pull these images and can create containers out of 
these images.  

68. Important docker commands?  
1. Docker ps (to see list of running containers)  
2. Docker ps -a (to see list of all containers)  
3. Docker images (to see list of all images)  
4. Docker run (to create docker container)  
4. Docker attach (to go inside container)  
6. Docker stop (to stop container)  
7. Docker start (to start container)  
8. Docker commit (to create image out of docker file)  
9. Docker rm (to delete container)  
10. Docker rmi (to delete image)  

69. What is Ansible?  
Ansible is one of the configuration Management Tools. It is a method 
through we automate system admin tasks. Configuration refers to 
each and every minute details of a system. If we do any changes in 
system means we are changing the configuration of a machine. That 
means we are changing the configuration of the machine. All 
windows/Linux system administrators manage the configuration of a 
machine manually. All DevOps engineers are managing this 
configuration automatic way by using some tools which are available 
in the market. One such tool is Ansible. That’s why we call Ansible as 
configuration management tool.  

70. Working process of Ansible?  
Here we crate file called playbook and inside playbook we write 
script in YAML format to create infrastructure. Once we execute this 
playbook, automatically code will be converted into Infrastructure. 
We call this process as IAC (Infrastructure as Code). We have open 
source and enterprise editions of Ansible. Enterprise edition we call 
Ansible Tower.  

71. The architecture of Ansible?  
We create Ansible server by installing Ansible package in it. Python 
is prerequisite to install ansible. We need not to install ansible 
package in nodes. Because, communication establishes from server to 
node through “ssh” client. By default all Linux machine will have 
“ssh” client. Server is going to push the code to nodes that we write in 
playbooks. So Ansible follows pushing mechanism.  

72. Ansible components?  
Server: – It is the place where we create playbooks and write code in 
YML format  
Node: – It is the place where we apply code to create infrastructure. 
Server pushes code to nodes.  
Ssh: – It is an agent through ansible server pushes code to nodes.  
Setup: – It is a module in ansible which gathers nodes information. 
Inventory file:- In this file we keep IP/DNS of nodes.  

73. Disadvantages in other SCM (Source Code Management) 
tools?  
• Huge overhead of Infrastructure setup  
• Complicated setup  
• Pull mechanism  
• Lot of learning required  

74. Advantages of Ansible over other SCM (Source Code 
Management) tools?  
• Agentless  
• Relies on “ssh”  
• Uses python  
• Push mechanism  

75. How does Ansible work?  
We give nodes IP addresses in hosts file by creating any group in 
ansible server why because, ansible doesn’t recognize individual IP 
addresses of nodes. We create playbook and write code in YAML 
script. The group name we have to mention in a playbook and then 
we execute the playbook. By default, playbook will be executed in all 
those nodes which are under this group. This is how ansible converts 
code into infrastructure.  

76. What do you mean by Ad-Hoc commands in Ansible?  
These are simple one liner Linux commands we use to meet 
temporary requirements without actually saving for later. Here we 
don’t use ansible modules. So there, Idempotency will not work with 
Ad-Hoc commands. If at all we don’t get required YAML module to 
write to create infrastructure, then we go for it. Without using 
playbooks we can use these Ad-Hoc commands for temporary 
purpose.  
Follow me for more DevOps Documents: Shivam Agnihotri 

77. Differences between Chef and Ansible?  
• Ansible chef  
• Playbook – Recipe  
• Module – Resource  
• Host – Node  
• Setup – Ohai  
• Ssh – Knife  
• Push-Pull  

78. What is Playbook in Ansible?  
Playbook is a file where we write YAML script to create infrastructure 
in nodes. Here, we use modules to create infrastructure. We create so 
many sections in playbook. We mention all modules in task section. 
You can create any no of playbooks. There is no limit. Each playbook 
defines one scenario. All sections begin with “-” & its attributes & 
parameters beneath it.  

79. Mention some list of sections that we mention in Playbook?  
1. Target section  
2. Task section  
3. Variable section  
4. Handler section  

80. What is Target section in Ansible playbook?  
This is one of the important sections in Playbook. In this section, we 
mention the group name which contains either IP addresses or 
Hostnames of nodes. When we execute playbook, then code will be 
pushed too all nodes which are there in the group that we mention in 
Target section. We use “all” key word to refer all groups.  

81. What is Task section in Ansible playbook?  
This is second most important section in playbook after target section. 
In this section, we are going to mention list of all modules. All tasks 
we mention in this task section. We can mention any no of modules in 
one playbook. There is no limit. If there is only one task, then instead 
of going with big playbook, simply we can go with arbitrary 
command where we can use one module at a time. If more than one 
module, then there is no option except going with big playbook.  

82. What is Variable section?  
In this section we are going to mention variables. Instead of hard 
coding, we can mention as variables so that during runtime it pulls the 
actual value in place of key. We have this concept in each and every 
programming language and scripting language. We use “vars” key 
word to use variables.  

83. What is Handler section?  
All tasks we mention in tasks section. But some tasks where 
dependency is there, we should not mention in tasks section. That is 
not good practice. For example, installing package is one task and 
starting service is one more task. But there is dependency between 
them. I.e. after installing package only, we have to start service. 
Otherwise it throws error. These kind of tasks, we mention in handler 
section. In above example, package task we mention in task section 
and service task we mention in handler section so that after installing 
task only service will be started.  

84. What is Dry run in playbook?  
Dry run is to test playbook. Before executing playbook in nodes, we 
can test whether the code in playbook is written properly or not. Dry 
run won’t actually executes playbook, but it shows output as if it 
executed playbook. Then by seeing the output, we can come to know 
whether the playbook is written properly or not. It checks whether the 
playbook is formatted correctly or not. It tests how the playbook is 
going to behave without running the tasks.  

85. Why are we using loops concept in Ansible?  
Sometimes we might need to deal with multiple tasks. For instance, 
Installing multiple packages, Creating many users, creation many 
groups..etc. In this case, mentioning module for every task is complex 
process. So, to address this issue, we have a concept of loops. We 
have to use variables in combination with loops.  

86. Where do we use conditionals in Playbooks?  
Sometimes, your nodes could be mixture of different flavors of Linux 
OS. Linux commands vary in different Linux operating systems. In this 
case, we can’t execute common set of commands in all machines, at 
the same time, we can’t execute different commands in each node 
separately. To address this issue, we have conditionals concept 
where commands will be executed based up on certain condition that 
we give.  

87. What is Ansible vault?  
Sometimes, we use sensitive information in playbooks like passwords, 
keys …etc. So any one can open these playbooks and get to know 
about this sensitive information. So we have to protect our playbooks 
from being read by others. So by using Ansible vault, we encrypt 
playbooks so that, those who ever is having password, only those can 
read this information. It is the way of protecting playbooks by 
encrypting them.  

88. What do you mean by Roles in Ansible?  
Adding more & more functionality to the playbooks will make it 
difficult to maintain in a single file. To address this issue, we organize 
playbooks into a directory structure called “roles”. We create 
separate file to each section and we just mention the names of those 
sections in playbook instead of mentioning all modules in main 
playbook. When you call main playbook, main playbook will call all 
sections files respectively in the order whatever order you mention in 
playbook. So, by using this Roles, we can maintain small playbook 
without any complexity.  

89. Write a sample playbook to install any package?  
— # My First YAML playbook  – 
hosts: demo user: 
ansible become: yes 
connection: ssh tasks:  – 
name: Install HTTPD 
on centos 7 action: yum 
name=httpd 
state=installed  

90. Write a sample playbook by mentioning variables instead of 
hard coding?  
— # My First YAML playbook  – 
hosts: demo user: 
ansible become: yes 
connection: ssh vars: 
pkgname: httpd tasks:  – 
name: Install HTTPD 
server on centos 7 action: 
yum name=‘{{pkgname}}’ 
state=installed  

91. What is CI & CD?  
CI means Continues Integration and CD means Continues 
Delivery/Deploy. Whenever developers write code, we integrate all 
that code of all developers at that point of time and we build, test and 
deliver/deploy to the client. This process we call CI & CD. Jenkins 
helps in achieving this. So instead of doing night builds, build as and 
when commit occurs by integrating all code in SCM tool, build, test 
and checking the quality of that code is what we call Continues 
Integration.  

92. Key terminology that we use in Jenkins?  
Integrate: Combine all code written by developers till some point of 
time.  
Build: Compile the code and make a small executable package.  
Test: Test in all environments whether application is working 
properly or not.  
Archived: Stored in an artifactory so that in future we may use/deliver 
again.  
Deliver: Handing the product to Client 
Deploy: Installing product in client’s 
machines.  

93. What is Jenkins Workflow?  
We attach Git, Maven, Selenium & Artifactory plug-ins to Jenkins. 
Once Developers put the code in Git, Jenkins pulls that code and send 
to Maven for build. Once build is done, Jenkins pulls that built code 
and send to selenium for testing. Once testing is done, then Jenkins 
will pull that code and send to Artifactory as per requirement and 
finally we can deliver the end product to client we call Continues 
delivery. We can also deploy with Jenkins into clients machine 
directly as per the requirement. This is what Jenkins work flow.  

94. What are the ways through which we can do Continues 
Integration?  
are total three ways through which we can do Continues 
Integration 1. Manually: – Manually write code, then do build 
manually and then test manually by writing test cases and deploy 
manually into clients machine.  
2. 
Scripts: – Can do above process by writing scripts so that these 
scripts do CI&CD automatically. But here complexity is, writing script 
is not so easy.  
3. 
Tool: – Using tools like Jenkins is very handy. Everything is 
preconfigured in these type of tools. So less manual intervention. This 
is the most preferred way.  

95. Benefits of CI?  
1. Detects bugs as soon as possible, so that bug will be rectified fast 
and development happens fast.  
2. Complete automation. No need manual intervention.  
3. We can intervene manually whenever we want. I.e. we can stop any 
stage at any point of time so have better control.  
4. Can establish complete and continues work flow.  
Follow me for more DevOps Documents: Shivam Agnihotri 

96. Why only Jenkins?  
• It has so many plug-ins.  
• You can write your own plug-in  
• You can use community plug-ins  
• Jenkins is not just a tool. It is a framework. I.e. you can do what 
ever you want. All you need is plug-ins.  
• We can attach slaves (nodes) to Jenkins master. It instructs 
others  
(slaves) to do Job. If slaves are not 
available, 
Jenkins itself does the job.  
• Jenkins also acts as crone server replacement. I.e. can do 
repeated tasks automatically  
• Running some scripts regularly E.g.: Automatic daily alarm.  
• Can create Labels (Group of slaves) (Can restrict where the 
project has to run)  

97. What is Jenkins Architecture?  
Jenkins architecture is Client-Server model. Where ever, we install 
Jenkins, we call that server is Jenkins master. We can create slaves 
also in Jenkins, so that, server load will be distributed to slaves. 
Jenkins master randomly assigns tasks to slaves. But if you want to 
restrict any job to run in particular slave, then we can do it so that, that 
particular job will be executed in that slave only. We can group some 
slaves by using “Label”  

98. How to install Jenkins?  
• You can install Jenkins in any OS. All OSs supports Jenkins. We 
access Jenkins through web page only. That’s why it doesn’t 
make any difference whether you install Jenkins in Windows or 
Linux.  
• Choose Long Term Support release version, so that you will get 
support from Jenkins community. If you are using Jenkins for 
testing purpose, you can choose weekly release. But for 
production environments, we prefer Long Term Support release 
version.  
• Need to install JAVA. Java is pre-requisite to install Jenkins.  
• Need to install web package. Because, we are going to access 
Jenkins through web page only.  

99. Does Jenkins open source?  
There are two editions in Jenkins  
1. Open source  
2. Enterprise edition  
Open source edition we call Jenkins. Here we get support from 
community if we need it.  
Enterprise edition we call Hudson. Here Jenkins company will 
provide support.  

100. How many types of configurations in Jenkins?  
There are total 3 types of configurations in Jenkins.  
1. Global: – Here, whatever configuration changes we do, 
applicable to whole Jenkins including jobs as well as nodes. This 
configuration has high priority. 2. Job: – These configurations 
applicable to only Jobs. Jobs also we call as projects or items in 
Jenkins.  
3. Node: – These configurations applicable to only nodes. Also we call 
Slaves. These are kind of helpers to Jenkins master to distribute the 
excessive load.  

101. What do you mean by workspace in Jenkins?  
The workspace is the location on your computer where Jenkins places 
all files related to the Jenkins project. By default each project or job is 
assigned a workspace location and it contains Jenkins-specific project 
metadata, temporary files like logs and any build artifacts, including 
transient build files. Jenkins web page acts like a window through 
which we are actually doing work in workspace.  

102. List of Jenkins services?  
• localhost:8080/restart (to restart Jenkins)  
• localhost:8080/stop (to stop Jenkins)  
• localhost:8080/start (to start Jenkins)  

103. How to create a free style project in Jenkins?  
• Create project by giving any name  
• Select Free style project  
• Click on build  
• Select execute windows batch command  
• Give any command (echo “Hello Dear Students!!”)  
• Select Save  
• Click on Build now  
• Finally can see Console output  

104. What do you mean by Plugins in Jenkins?  
• With Jenkins, nearly everything is a plugin and that nearly all 
functionality is provided by plugins. You can think of Jenkins as 
little more than an executor of plugins.  
• Plugins are small libraries that add new abilities to Jenkins and 
can provide integration points to other tools.  
• Since nearly everything Jenkins does is because of a plugin, 
Jenkins ships with a small set of default plugins, some of which 
can be upgraded independently of Jenkins  

105. How to create Maven Project?  
• Select new item  
• Copy the git hub maven project link and paste in git section in 
Jenkins  
• Select build  
• Click on clean package  
• Select save  
• Click on Build now  
• Verify workspace contents with GitHub sideSee console output  

106. How can we Schedule projects?  
Sometimes, we might need some jobs to be executed after frequent 
intervals.  
To schedule a job,  
• Click on any project  
• Click on Configure  
• Select on Build triggers  
• Click on Build periodically  
• Give timing (* * * * *)  
• Select Save  
• Can see automatic builds every 1 min  
• You can manually trigger build as well if you want  

107. What do you mean by Upstream and Downstream projects?  
We can also call them as linked projects. These are the ways through 
which, we connect jobs one with other. In Upstream jobs, first job will 
trigger second job after build is over. In Downstream jobs, second 
job will wait till first job finishes its build. As and when first job 
finishes its work, then second job will be triggered automatically. In 
Upstream, first job will be active. In Downstream jobs, second job will 
be active. We can use any one type to link multiple jobs.  

108. What is view in Jenkins?  
We can customize view as per our needs. We can modify Jenkins 
home page. We can segregate jobs as per the type of jobs like free 
style jobs and maven jobs and so on. To create custom view  
• Select List of Related Projects  
• Select Default views  
• Click on All  
• Click on + and select Freestyle  
• Select List Views  
• Select Job filter  
• Select required jobs to be segregated  
• Now, you can see different view  

109. What is User Administration in Jenkins?  
In Jenkins, we can create users, groups and can assign limited 
privileges to them so that, we can have better control on Jenkins. 
Users will not install Jenkins in their machines. They access Jenkins as 
a user. Here we can’t assign permissions directly to users. Instead we 
create “Roles” and assign permissions to those roles. These roles we 
attach to users so that users get the permissions whatever we assign 
to those roles.  

110. What is Global tool configuration in Jenkins?  
We install Java, Maven, Git and many other tools in our server. 
Whenever Jenkins need those tools, by default Jenkins will install 
them automatically every time. But it’s not a good practice. That’s why 
we give installed path of all these tools in Jenkins so that whenever 
Jenkins need them, automatically Jenkins pull them form local 
machine instead of downloading every time. This way of giving path 
of these tools in Jenkins we call “Global tool configuration”  

111. What is Build?  
Build means, Compile the source code, assembling of all class files 
and finally creating deliverable  
Compile: – Convert Source code into machine-readable format  
Assembly (Linking): – Grouping all class files  
Deliverable: – .war, .jar  
The above process is same for any type of code. This process we call 
Build.  

112. What is Maven?  
Maven is one of the Build tools. It is the most advance build tool in the 
market.  
In this, everything is already pre-configured. Maven belongs to 
Apache Company. We use maven to build Java code only. We can’t 
build other codes by using Maven. By default, we get so many plugins 
with Maven. You can write your own plug-in as well. Maven’s local 
repository is “.M2” where we can get required compilers and 
dependencies. Maven’s main configuration file is “pom.xml” where 
we keep all instructions to build.  

113. Advantages of Maven?  
• Automated tasks (Mention all in pom.xml)  
• Multiple Tasks at a time  
• Quality product  
• Minimize bad builds  
• Keep history  
• Save time – Save money  
• Gives set of standards  
• Gives define project life cycle (Goals)  
• Manage all dependencies  
• Uniformity in all projects  
• Re-usability  

114. List of Build tools available in Market?  
• C and C++: Make file  
• .Net: Visual studio  
• Java: Ant, Maven  

115. What is the architecture of Maven?  
Main configuration file is pom.xml. For one project, there will be one 
workspace and one pom.xml  
Requirements for build: –  
• Source code (Will be pulled from Git hub)  
• Compiler (Pulls from remote repo and then put them in local 
repo, from there, maven pulls into Workspace)  
• Dependencies (Pulls from remote repo and then put them in 
local repo, from there, maven pulls into Workspace)  

116. What is Maven’s Build Life Cycle?  
In maven, we have different goals. These are  
• Generate resources (Dependencies)  
• Compile code  
• Unit test  
• Package (Build)  
• Install (in to local repo & artifactory)  
• Deploy (to servers)  
• Clean (delete all run time files)  

117. What does POM.XML contains?  
POM.XML is maven’s main configuration file where we keep all 
details related to project. It contains  
• Metadata about that project  
• Dependencies required to build the project  
• The kind of project  
• Kind of output you want (.jar, .war)  
• Description about that project  

118. What is Multi-Module Project in Maven?  
• Dividing big project into small modules, we call Multi Module 
Project.  
• Each module must have its own SRC folder & pom.xml so that 
build will happen separately  
• To build all modules with one command, there should be a 
parent pom.xml file. This calls all child pom.xml files 
automatically  
• In parent pom.xml file, need to mention the child pom.xml files 
in an order.  

119. What is Nagios?  
Nagios is one of the monitoring tools. By using Nagios we can monitor 
and give alerts. Where ever you install Nagios that becomes Nagios 
server. Monitoring is important, because we need to make sure that 
our servers should never go down. If at all in some exceptional cases 
server goes down, immediately we need alert in the form of 
intimation so that we can take required action to bring the server up 
immediately. So for this purpose, we use Nagios.  

120. Why do we have to use Nagios?  
There are many advantages in using Nagios  
• It is oldest & Latest (every now and then, it is getting upgraded 
as per current market requirements)  
• Stable (we have been using this since so many years and it is 
performing well)  
• By default, we get so many Plug-ins  
• It is having its own Database  
• Nagios is both Monitoring & Alerting tool.  

121. How does Nagios works?  
• We mention all details in configuration files what data to be 
collected from which machine  
• Nagios daemon reads those details about what data to be 
collected  
• Daemon use NRPE (Nagios Remote Plug-in Executer) plug-in to 
collect data form nodes and stores in its own database  
• Finally displays in Nagios dashboard  

122. What is the Directory structure of Nagios?  
/usr/local/nagios/bin – binary files  
/usr/local/nagios/sbin – CGI files (to get web page)  
/usr/local/nagios/libexec – plugins  
/usr/local/nagios/share – PHP Files  
/usr/local/nagios/etc – configuration files  
/usr/local/nagios/var – logs  
/usr/local/nagios/var/status.dat(file) – database  

123. What are the Important Configuration files in Nagios?  
Nagios main configuration file is 
/usr/local/nagios/etc/nagios.cfg  
/usr/local/nagios/etc/objects/localhost.cfg (where we keep hosts 
information)  
/usr/local/nagios/etc/objects/contacts.cfg (whom to be informed 
(emails))  
/usr/local/nagios/etc/objects/timeperiods.cfg (at what time to 
monitor)  
/usr/local/nagios/etc/objects/commands.cfg (plugins to use)  
/usr/local/nagios/etc/objects/templates.cfg (sample templates)

**Questions and Answers**
 Q1. What is DevOps, and how does it enhance
 collaboration between development and operations
 teams?
 DevOps is a set of practices aiming to automate and improve the
 relationship between software development and IT operations. It
 fosters collaboration, communication, and integration throughout
 the development lifecycle.

 Q2. Explain the concept of Continuous Integration
 (CI) and its role in DevOps.
 CI involves regularly merging code changes into a shared
 repository. It helps identify and fix integration issues early,
 ensuring a more stable and reliable codebase.

 Q3. What is Continuous Deployment (CD) in DevOps?
 CD automates the process of deploying code changes to
 production after passing automated tests. It allows for faster and
 more frequent releases.


 Q4. How does version control contribute to DevOps
 practices?
 Version control, like Git, helps track changes in code, collaborate
 efficiently, and roll back to previous states if needed. It ensures
 code integrity and collaboration among team members.
 
 Q5. What are containers, and how do they facilitate
 DevOps workflows?
 Containers encapsulate applications and their dependencies,
 ensuring consistency across different environments. Tools like
 Docker enable easy deployment and scalability.
 
 Q6. Explain the importance of Infrastructure as Code
 (IaC) in DevOps.
 IaC allows the automation of infrastructure provisioning through
 code, promoting consistency, repeatability, and efficient resource
 management.

 Q7. How does automated testing contribute to the
 DevOps pipeline?
 Automated testing ensures code quality and reduces the risk of
 bugs by running tests automatically whenever code changes are
 made.

 Q8. What is the purpose of monitoring and logging in
 a DevOps environment?
 Monitoring and logging help detect and diagnose issues in real
time, ensuring the reliability and performance of applications.

 Q9. Explain the concept of Microservices and how it
 aligns with DevOps.
 Microservices architecture breaks down applications into smaller,
 independent services, aligning with DevOps principles by enabling
 agility, scalability, and easier maintenance.

 
 Q10. How does DevOps address security concerns in
 the software development lifecycle?
 DevSecOps integrates security practices throughout the
 development process, ensuring that security is not a bottleneck
 but an integral part of the workflow.

 Q11. What role does the DevOps toolchain play in the
 development process?
 The DevOps toolchain comprises tools for coding, building, testing,
 and deploying applications. It streamlines the development
 process, making it more efficient.

Q12. How do you manage configuration in a DevOps
 environment?
 Configuration Management tools, like Ansible or Puppet, automate
 the setup and maintenance of servers and infrastructure, ensuring
 consistency and reducing errors.

 Q13. Explain the difference between Blue-Green
 Deployment and Canary Deployment.
 In Blue-Green, two identical environments are maintained, with
 only one active. In Canary, new features are gradually rolled out to
 a subset of users, allowing for testing before a full release.

 Q14. What is GitOps, and how does it relate to
 DevOps practices?
 GitOps is a set of practices that use Git repositories as the source
 of truth for infrastructure and application deployments. It
 promotes versioning, auditability, and collaboration.

 Q15. How does DevOps contribute to the concept of
 "Infrastructure as a Service" (IaaS)?
 DevOps practices enhance IaaS by automating infrastructure
 provisioning, allowing for scalability and flexibility in resource
 management.

  Q16. Describe the concept of "Shift-Left" in the
 context of DevOps.
 "Shift-Left" refers to the practice of addressing issues early in the
 development process, promoting early testing, and reducing the
 cost and impact of fixing issues later in the lifecycle.

 Q17. What are the key benefits of adopting a DevOps
 culture?
 Benefits include faster delivery of features, improved
 collaboration, reduced time-to-market, and increased overall
 efficiency in software development.
 
 Q18. How do you handle dependencies in a DevOps
 pipeline?
 Dependency management tools, like Maven or npm, help ensure
 that the required libraries and components are consistently used
 across different environments.

 Q19. Explain the concept of a "Pipeline as Code" in
 DevOps.
 "Pipeline as Code" involves defining and managing the deployment
 pipeline through code, enabling version control, collaboration, and
 automation of the entire process.

 Q20. What is the significance of automated rollback in
 a DevOps pipeline?
 Automated rollback ensures that, in the event of a deployment
 failure, the system can automatically revert to the previous
 version, minimizing downtime and impact on users.

 Q21. How does DevOps contribute to scalability and
 resource optimization?
 DevOps practices, such as automation and containerization, enable
 organizations to scale resources efficiently, adapting to varying
 workloads while optimizing costs.

 Q22. Explain the term "Chaos Engineering" in the
 context of DevOps.
 Chaos Engineering involves intentionally introducing failures into a
 system to observe how it responds. This helps identify weaknesses
 and improve overall system resilience.

 Q23. What is the role of a Configuration Management
 Database (CMDB) in DevOps?
 A CMDB stores information about configuration items, facilitating
 efficient management and tracking of changes in the IT
 environment.

 Q24. How does DevOps foster a culture of continuous
 learning and improvement?
 DevOps encourages a culture of continuous feedback, learning
 from failures, and iteratively improving processes, leading to
 enhanced productivity and innovation.

 Q25. Explain the concept of "Infrastructure as Code"
 (IaC) in simple terms.
 IaC is like using a recipe to cook – you define how your
 infrastructure should look in code, and tools use this code to
 automatically set up and manage your servers and resources.
 
 Q26. How do you ensure security in a containerized
 environment in a DevOps setup?
 Security in a containerized environment involves scanning
 container images for vulnerabilities, restricting permissions, and
 implementing best practices for secure container deployment.

 Q27. What are the key principles of DevOps that
 teams should follow?
 Key principles include collaboration, automation, continuous
 integration, continuous delivery, and a culture of shared
 responsibility between development and operations teams.
 
Q28. How does DevOps contribute to achieving a
 faster time-to-market for software products?
 DevOps automates processes, reduces manual interventions and
 promotes collaboration, enabling faster development, testing and
 deployment cycles, leading to a quicker time-to-market.

 Q29. What is the role of a Version Control System
 (VCS) in a DevOps environment?
 A VCS tracks changes to code over time, allowing multiple
 developers to work on a project simultaneously, merge their
 changes, and maintain a versioned history of the codebase.
 
 Q30. How does DevOps handle database changes and
 migrations?
 DevOps practices include automating database changes and
 migrations through tools like Liquibase or Flyway, ensuring
 consistency and reliability in database deployments.

Q31. Explain the role of a Build Tool in DevOps and
 provide an example.
 Build tools, like Apache Maven, automate the compilation and
 packaging of code, streamlining the build process in DevOps
 workflows.

 Q32. What are the core principles of Continuous
 Monitoring in a DevOps environment?
 Continuous Monitoring involves tracking performance, availability,
 and security metrics in real-time, ensuring early detection and
 resolution of issues.

 Q33. How does a DevOps approach address the
 challenges of manual testing, especially in large-scale
 projects?
 DevOps promotes automated testing, reducing manual efforts,
 improving accuracy, and allowing faster feedback on code
 changes.

 Q34. What is the role of a Container Registry in the
 context of containerized applications?
 A Container Registry stores and manages container images,
 facilitating version control, security, and efficient distribution in a
 DevOps environment.
 
 Q35. Explain the concept of "Immutable
 Infrastructure" and its advantages in a DevOps setup.
 Immutable Infrastructure involves replacing and redeploying
 entire infrastructure components instead of making changes in
place, promoting consistency, reliability, and easier rollbacks.

 Q36. How does Feature Toggle contribute to a more
 flexible and controlled release process in DevOps?
 Feature Toggles allow developers to enable or disable specific
 features at runtime, providing control over feature releases and
 minimizing the impact of changes.

 
 Q37. What is the purpose of a Deployment Pipeline in
 a DevOps workflow?
 A Deployment Pipeline automates the steps from code commit to
 production deployment, ensuring consistent and reliable delivery
 of software.

 Q38. Explain the concept of "Infrastructure as
 Versioned Code" in a DevOps environment.
 Infrastructure as Versioned Code involves managing infrastructure
 configurations with version control, ensuring traceability,
 repeatability, and easy collaboration.

 Q39. How does DevOps contribute to optimizing the
 use of cloud resources in a cost-effective manner?
 DevOps practices enable auto-scaling, resource tagging, and
 efficient use of cloud services, ensuring cost optimization without
 compromising performance.
 
Q40. Explain the concept of "Dark Launching" in the
 context of feature deployment.
 Dark Launching involves releasing a feature to a limited audience
 before making it globally available, allowing for testing and
 validation in a real-world scenario.

 Q41. What is the role of a Package Manager in DevOps
 and how does it simplify software distribution?
 A Package Manager automates the process of installing, updating,
 and managing software packages, streamlining the distribution of
 dependencies.
 
 Q42. How do you ensure security and compliance in a
 multi-cloud DevOps environment?
 DevOps practices include security scanning, policy enforcement,
 and automation to ensure consistent security and compliance
 across multiple cloud platforms.

 Q43. Explain the concept of "Shift-Right" testing in
 DevOps.
 "Shift-Right" testing involves performing testing in a production
like environment, capturing real-world scenarios and improving the
 overall quality of applications.
 
 Q44. What are the key metrics to monitor in a DevOps
 environment for performance optimization?
 Key metrics include response time, error rate, resource utilization,
 and throughput, providing insights into the performance and
 health of applications.
 
 Q45. Explain the role of a Configuration Management
 Tool in automating infrastructure changes.
 Configuration Management Tools automate the setup and
 maintenance of infrastructure, ensuring consistency and reducing
 manual errors in a DevOps environment.

 Q46. What is the purpose of a Canary Analysis in the
 context of deployment strategies?
 Canary Analysis involves releasing a new version of an application
 to a small subset of users and gradually expanding, allowing for
 monitoring and early detection of issues.

 Q47. Explain the concept of "Fail Fast" in DevOps and
 its impact on software development.
 "Fail Fast" encourages early detection and correction of issues,
 minimizing the impact of failures and accelerating the overall
 development process.

 Q48. What role does a Continuous Feedback loop play
 in a DevOps culture?
 Continuous Feedback involves gathering insights from users,
 stakeholders, and automated processes, facilitating continuous
 improvement and alignment with business goals.

 Q49. How do you ensure traceability and auditability
 in a DevOps environment?
 DevOps practices include version control, documentation, and
 logging to ensure traceability and auditability of changes made
 throughout the development lifecycle.
 
 Q50. What is the significance of a "Blameless Post
Mortem" in a DevOps culture?
 A Blameless Post-Mortem focuses on learning from failures
 without assigning blame, encouraging a culture of continuous
 improvement and shared responsibility.
 Follow for more amazing


